<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ios | Marcin Pędzimąż]]></title>
  <link href="http://noxytrux.github.io/blog/categories/ios/atom.xml" rel="self"/>
  <link href="http://noxytrux.github.io/"/>
  <updated>2014-12-30T21:25:21+01:00</updated>
  <id>http://noxytrux.github.io/</id>
  <author>
    <name><![CDATA[Marcin Pędzimąż]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Advanced iOS animations]]></title>
    <link href="http://noxytrux.github.io/blog/2014/12/26/advanced-ios-animations/"/>
    <updated>2014-12-26T17:27:15+01:00</updated>
    <id>http://noxytrux.github.io/blog/2014/12/26/advanced-ios-animations</id>
    <content type="html"><![CDATA[<p>This post will be probably very long and will cover a lot about making your app as juicy as possible. Best iOS apps have heavily animated UI and guide users by themselves so today we will talk about how to take advantages of all new xCode features, and how design app or how to use eyes in terms of imagining your app being animated.</p>

<!--more-->


<p>There are some very basic skills that you need to become iOS dev, good knowing of API is a 80% of your success to become good iOS developer. Old wolfs who were starting from 3.2 know what kind of hell we actually have to pass to get today&rsquo;s ARC, IB improvements and much more. But there is something that can make you become one of those ninjas who put his &ldquo;wow&rdquo; effect to every app they touch. Today I will show you how to convert simple record app to something breathtaking. It will be first part of record app tutorial connected with Core Audio (I will talk about CA next time.) but today we focus only on design, interface builder and last but not least animations of user interface.</p>

<p>There are some things that you actually need to learn if you want to survive in iOS world and most of all become a SWAT team member of iOS developers, those things are:</p>

<ul>
<li>Learn GIMP or Photoshop</li>
<li>If you don&rsquo;t want to learn photoshop at least you should know how to import colors, fonts etc.</li>
<li>Buy notepad and some pencils.</li>
<li>Spend a lot time experimenting with UIAnimations and Gesture Recognizers</li>
<li>Read documentation! (even those old 4.X API)</li>
<li>Attend to Hackatons.</li>
<li>Fail a lot :) without that you cannot learn</li>
<li>Take criticism (this one is probably most important)</li>
<li>Learn OpenGL</li>
<li>And much more ;)</li>
</ul>


<p>More time you spend on all of this less time you will need to build an iOS app. Look at the video below i build a project for this post in one day and put even to much animations but i want to show you how to make your app feel alive, and tell user he can interact with it. It gives this feeling of &ldquo;doing something&rdquo; all the time.</p>

<p><div class="embed-video-container"><iframe src="http://www.youtube.com/embed/y9o2fGH6oYo" allowfullscreen></iframe></div></p>

<p>So how i achieve this ? Most of all hard work but there are some key things that will allow you to do the same ;)</p>

<p><b>GIMP</b></p>

<p>Knowing at least one graphic editor will let you save a lot of time and work. Even if you are not a master chief graphic designer with good hand. You can still at least modify or draw your own simple assets. And most important if your UI guy is on vacation and you just need to resize asset, or put some alpha on it you can do it by yourself, common it&rsquo;s not a rocket science.</p>

<p><b>PHOTOSHOP</b></p>

<p>More advanced tool and not cheap but most companies have it so you can still sit with graphic and look how he/she do some stuff or ask him/her how to use some basic functions. e.g: most design are done in photoshop so instead of getting UI image and some guides why not to ask to get PSD so you can import all directly from PSD to xCode by e.g: simply cmd+c/v hex colors or font names and sizes? less work for designer, and for me it&rsquo;s way more elegant and faster to implement.</p>

<p><b>INTERFACE BUILDER</b></p>

<p>Long time ago there was an argument if it&rsquo;s really useful and safe to use IB but today when we have autolayout ? As George Carlin say&rsquo;s - &ldquo;It&rsquo;s bullshit and it&rsquo;s bad for you&rdquo; and i cannot agree more. User IB as much as possible!. There are now so many COOL features that may dramatically speed up your development and decrease lines of code. And as we know less code - better to maintain, debug and of course less bugs. Some features that can save a lot of our work:</p>

<p><code>IB_DESIGNABLE</code> - new feature that allow you to see a preview of your custom control in IB that&rsquo;s a game changer because it&rsquo;s allow you to see some of your work without even opening app on device or simulator.</p>

<p>Take a look IB without designable:</p>

<p><img class="center" src="/assets/images/no_designable.png" width="800"></p>

<p>And with enabled designable controls preview:</p>

<p><img class="center" src="/assets/images/designable.png" width="800"></p>

<p><code>Interface Builder Preview</code> - super cool feature, and one of the most wanted from the time autolayout was introduced. Simply this allow you to see preview of your app layout on ALL selected devices. Again no need to run app and you can see if there are no glitches in your autolayout UI.</p>

<p><img class="center" src="/assets/images/ib_preview.png" width="800"></p>

<p><code>Assets catalog</code> - Now when we have new iPhones with @3x resolution assets catalog is only place when you can manage all of assets without getting mad. Also it allow you to FINALLY set rendering mode of asset! no more:</p>

<pre><code class="objc">imageView.image = [[UIImage imageNamed:@"myasset"] imageWithRenderingMode:UIImageRenderingModeAlwaysTemplate];
</code></pre>

<p>Also allow you to manage <code>Size Classes</code> And here how it looks:</p>

<p><img class="center" src="/assets/images/image_assets.png" width="800"></p>

<p>Those are one of most used things that will save your day many times. There is of course a lot more but it&rsquo;s time to get to our main topic which is:</p>

<p><b>ANIMATIONS</b></p>

<p>Without them your app will be at least average if you really want to make your app look epic put as much animations as you can, your app need to live otherwise it will die in all those apps that spam appstore day by day</p>

<p><img class="center" src="/assets/images/deal_with_it.gif" width="800"></p>

<p>But before we start, you need to learn how to &ldquo;see&rdquo; animations in your app. I&rsquo;m always doing sketches so I can visualize how may app will behave like this one:</p>

<p><img class="center" src="/assets/images/sketch_app.jpg" width="800"></p>

<p>If you are lucky and you have designer he can always do it for you or even make some animation using AfterEffects and export to gif or some video. It&rsquo;s always a good thing to have something like this so you will not lose the &ldquo;idea&rdquo; of animation. Ok enough of talking time for some code!</p>

<p>First let&rsquo;s talk how I did this idle spinner animation when app is not used. I saw this kind of loading spinner in some games and I decided to do the same here so app give that feeling of &ldquo;doing something&rdquo; while not used. So first I put some multiple <code>UIView</code> in my container as the each one of them will be animated using simple rotation of it&rsquo;s center</p>

<p>Drawing:</p>

<pre><code class="objc">- (void)drawRect:(CGRect)rect {

    //get current context
    CGContextRef context = UIGraphicsGetCurrentContext();

    //this code moves our circle along view bounds if you decide not to center it.
    CGContextSaveGState(context);
    CGContextTranslateCTM(context, self.offsetX, self.offsetY);

    //now create a path that will be used for drawing
    CGMutablePathRef arc = CGPathCreateMutable();

    //this function retuns path representing arc (piece of pie)
    CGPathAddArc(arc, NULL,
                 rect.size.width * 0.5f, rect.size.height * 0.5f,
                 self.radius,
                 degreeToRadian(self.startAngle), //see we conver deegree to radians here by deg*M_PI/180.0
                 degreeToRadian(self.endAngle),
                 YES); //clock wise

    //now we need to stroke our arc 
    CGPathRef strokedArc =
    CGPathCreateCopyByStrokingPath(arc, NULL,
                                   self.lineWidth,
                                   kCGLineCapButt,
                                   kCGLineJoinMiter,
                                   10.0f);

    //apply path for drawing
    CGContextAddPath(context, strokedArc);
    //setup colors
    CGContextSetFillColorWithColor(context, self.fillColor.CGColor);
    CGContextSetStrokeColorWithColor(context, self.strokeColor.CGColor);
    //and draw
    CGContextDrawPath(context, kCGPathFillStroke);
    CGContextRestoreGState(context);
}
</code></pre>

<p>This will give us nicely done arcs in our IB of course we are declaring our view as:</p>

<pre><code class="objc">@import UIKit;

IB_DESIGNABLE

@interface ADVArcRotationView : UIView

@property (nonatomic) IBInspectable CGFloat offsetX;
@property (nonatomic) IBInspectable CGFloat offsetY;
@property (nonatomic) IBInspectable CGFloat radius;

@property (nonatomic) IBInspectable UIColor *fillColor;
@property (nonatomic) IBInspectable UIColor *strokeColor;
@property (nonatomic) IBInspectable CGFloat lineWidth;

@property (nonatomic) IBInspectable CGFloat startAngle;
@property (nonatomic) IBInspectable CGFloat endAngle;

@end
</code></pre>

<p>So we can see live preview in Interface builder, see <code>IBInspectable</code> ? this allow you to declare variables available from interface builder so you can modify it look without compiling.</p>

<p>Ok so how to animate the views ? there are actually two ways of doing that one by using <code>UIView animateWithDuration</code> or <code>CABasicAnimation</code> from Core Animation. I want to focus today mostly on Core Animation so i will show some simple UIView animations later on, but for now a bit about Core Animation. It&rsquo;s one of the low level frameworks provided by Apple allowing to create animations that can be apply directly on views layers, notice that we are not working on UIView anymore we are going deeper by playing with UIView layer property. <code>CALayer</code> is a lightweight entity used to acutally do drawing for you under the hood. Layer can have sublayers so it’s pretty similar to working with views. The main difference is that you can do some animated drawing which can&rsquo;t be achieved by using UIView  <code>drawRect:</code> method. You can even create your own fully custom layers. Of course there is some magic going under the hood for you. If you read documentation you will notice there is something called presentationLayer this guy hold current view state while animating and this may be useful in some cases. This is also a good moment to tell why iOS works so damn fast, and why doing this kind of animations on Android make some of developers working on it commit suicide; the answer is simple:</p>

<p><b>iOS perform hardware accelerated animations</b> - what does this mean? iOS rendering pipeline is based on OpenGL and its directly connected to whole API (right now they probably doing some Metal SDK transitions to speed up even more some stuff) That&rsquo;s why every thing animated in iOS works so smooth.</p>

<p>Ok time to explain how do we actually animate idle arcs:</p>

<pre><code class="objc">
//collect views from IB using collections

@interface ADVIdleViewController ()

@property (strong, nonatomic) IBOutletCollection(ADVArcRotationView) NSArray *arcViews;

@end
</code></pre>

<p>Creating rotation animation:</p>

<pre><code class="objc">-(CAKeyframeAnimation *)generateLayerAnimationWithDuration:(CGFloat)duration
                                                   reverse:(BOOL)reverse
{
    //create animation that will modify "transform" property of layer iOS allow to use some build in keywords
    //like "rotation" or "scale" to build for us whole functions that will interpolate values.

    CAKeyframeAnimation *rotationAnimation = [CAKeyframeAnimation animationWithKeyPath:@"transform.rotation"];

    NSArray *rotationValues = reverse ? @[@(2.0f * M_PI), @0.0f] : @[@0.0f, @(2.0f * M_PI)];

    [rotationAnimation setValues:rotationValues];

    rotationAnimation.repeatCount = arc4random() % kADVMaximumAnimationRepeatCount;
    //by default animation is removed from layer after finish but we want actually want to keep it so we can find it later.
    rotationAnimation.removedOnCompletion = NO; 
    rotationAnimation.duration = duration;
    rotationAnimation.delegate = self;

    return rotationAnimation;
}
</code></pre>

<p>Applying animation to particular view layer:</p>

<pre><code class="objc">
- (void)setupArcsRoationAnimationsd
{
    for (NSUInteger i = 0; i&lt;self.arcViews.count; i++) {

        UIView *v = self.arcViews[i];

        CAKeyframeAnimation *anim = [self generateLayerAnimationWithDuration: (arc4random() % 5) + kADVMinimumAnimationDuration
                                                                     reverse: ((i == 3 || i == 5))];

        [v.layer addAnimation:anim
                       forKey:kADVAnimationKey];
    }
}
</code></pre>

<p>Ok now because we set us as delegate to animation we have to handle it in this function:</p>

<pre><code class="objc">//get notified that animations is finished
- (void)animationDidStop:(CAAnimation *)anim
                finished:(BOOL)flag
{

    //loop by all views that are attached
    for (UIView *v in self.arcViews) {

        //and get animation for this view layer
        CAAnimation *current = [v.layer animationForKey:kADVAnimationKey];

        //check if the animation that just complete is the one of the view we are looking for
        if (current == anim) {

            //if so now we can remove it 
            [v.layer removeAnimationForKey:kADVAnimationKey];

            //and setup new one so the whole view will endlessly animates
            CAKeyframeAnimation *anim = [self generateLayerAnimationWithDuration: (arc4random() % 5) + kADVMinimumAnimationDuration
                                                                         reverse: (arc4random() % 2) ];

            [v.layer addAnimation:anim
                           forKey:kADVAnimationKey];

            break;
        }
    }
}
</code></pre>

<p>For example you can also use regular UIView animation that&rsquo;s how I manage to do this magic echo impulse ring that blinks all the time:</p>

<pre><code class="objc">
//get view from IB
@property (weak, nonatomic) IBOutlet ADVArcRotationView *impulseView;

//animate using regular UIView animation

- (void)setupImpulseViewAnimation
{
    //break retain cycle, and safeguard for view release
    __weak __typeof__(self) weakSelf = self;

    //initial setup hiden super small view
    self.impulseView.alpha = 0.0f;
    self.impulseView.transform = CGAffineTransformMakeScale(0.0001, 0.0001);

    //firts part
    [UIView animateWithDuration:0.7
                          delay:0.0
                        options:UIViewAnimationOptionCurveLinear
                     animations:^{

                         //scale it to 80% of size and at this time make it visible 
                         weakSelf.impulseView.alpha = 1.0;
                         weakSelf.impulseView.transform = CGAffineTransformMakeScale(0.8, 0.8);

                     } completion:^(BOOL finished) {

                         //second part
                         [UIView animateWithDuration:0.3
                                               delay:0.0
                                             options:UIViewAnimationOptionCurveLinear
                                          animations:^{

                                              //by the rest time fade it out again and scale to 100% of size
                                              weakSelf.impulseView.alpha = 0.0f;
                                              weakSelf.impulseView.transform = CGAffineTransformMakeScale(1.0, 1.0);

                                          } completion:^(BOOL finished) {

                                              //recursive animation calling itself
                                              [weakSelf setupImpulseViewAnimation];
                                          }];
                     }];

}
</code></pre>

<p>Doing heartbeat animation is also very simple again we will use <code>CABasicAnimation</code> and the <code>autoreverses</code> property of animation. This will simply reverse animation to its original state automatically.</p>

<pre><code class="objc">- (void)applyHeartBeatAnimation:(BOOL)apply
{
    if (!apply) {

        [self.layer removeAnimationForKey:kADVHeartBeatAnimationKey];

        return;
    }

    CABasicAnimation *heartBeatAnimation;

    heartBeatAnimation = [CABasicAnimation animationWithKeyPath:@"transform.scale"];
    heartBeatAnimation.duration = 0.5;
    heartBeatAnimation.repeatCount = INFINITY;
    heartBeatAnimation.autoreverses = YES;
    heartBeatAnimation.fromValue = @1.0f;
    heartBeatAnimation.toValue = @1.15f;

    [self.layer addAnimation:heartBeatAnimation
                      forKey:kADVHeartBeatAnimationKey];
}
</code></pre>

<p>When it comes to simple animations that gives wow effect, the one of the most important places in your app is it&rsquo;s menu.
If you build cool and fancy menu users will way more gladly use it even without doing anything just to play with it.</p>

<p>My menu uses new iOS7 feature: animation with spring to be more precise:</p>

<pre><code class="objc">+ (void)animateWithDuration:(NSTimeInterval)duration 
                      delay:(NSTimeInterval)delay 
     usingSpringWithDamping:(CGFloat)dampingRatio 
      initialSpringVelocity:(CGFloat)velocity 
                    options:(UIViewAnimationOptions)options 
                 animations:(void (^)(void))animations 
                 completion:(void (^)(BOOL finished))completion;
</code></pre>

<p>This animations apply spring effect to your animation which in many cases looks super fancy and cool and you can combine this with any other animation it&rsquo;s just something you get for free so why not to use it? the code:</p>

<pre><code class="objc">- (void)openActionMenu
{
    //reset buttons positions
    for (UIButton *btn in self.menuButtons) {

        btn.center = self.view.center;
    }

    //if you need proper order of objects from IB and you use OutletCollections which do not guarantee proper order
    //you may consider of sorting them somehow in this case i simply use tag propety of view to get order i want.

    NSSortDescriptor *sortDesc = [NSSortDescriptor sortDescriptorWithKey:@"tag"
                                                               ascending:YES];

    NSArray *sortedButtons = [self.menuButtons sortedArrayUsingDescriptors:@[sortDesc]];

    CGFloat delay = 0.1f;

    //desired buttons location along center
    NSArray *constraintValues = @[ @[@0, @0],
                                   @[@0, @120],
                                   @[@(-110), @50],
                                   @[@(-80), @(-90)],
                                   @[@80, @(-90)],
                                   @[@110, @50]];

    for (NSUInteger i=0; i&lt;sortedButtons.count; ++i) {

        UIButton *btn = sortedButtons[i];
        NSArray *values = constraintValues[i];

        //go button by button and animate alpha so they become visible in 1/4 time of animation
        [UIView animateWithDuration:0.2
                              delay:delay
                            options:UIViewAnimationOptionAllowUserInteraction
                         animations:^{

                             btn.alpha = 1.0f;

                         } completion:^(BOOL finished) {

                         }];

        //notice we can combine multiple animations to one view as long as they touch different properites
        [UIView animateWithDuration:0.8
                              delay:delay
             usingSpringWithDamping:0.2
              initialSpringVelocity:0.05
                            options:UIViewAnimationOptionBeginFromCurrentState
                         animations:^{

                             //apply new position
                             CGPoint newCenter = btn.center;
                             newCenter.x += [values[0] floatValue];
                             newCenter.y += [values[1] floatValue];

                             btn.center = newCenter;

                         } completion:^(BOOL finished) {

                         }];

        delay += 0.1;
    }
}
</code></pre>

<p>Ok we are done with those easy ones now time for some tricky animations. We left two of them, the hole animation that make record button empty inside and arc animation that represent record time animation. (e.g: we get user desired time of recording and he need to fit this time before it run out.)</p>

<p>Each <code>CALayer</code> contains mask property which tells him what could be actually visible. So simply whatever shape you will apply on your layer using this mask will become transparent. In our case we actually need the opposite we want to tell our view what we want to keep not what we want to cut.</p>

<pre><code class="objc">
//mask whole view so at this point it will be fully transparent
UIBezierPath *basePath = [UIBezierPath bezierPathWithRect:realBounds];

//now prepare path that contain circle as a mask

//empty mask path
UIBezierPath *startPath = [UIBezierPath bezierPathWithOvalInRect:CGRectMake(viewSize*0.5, viewSize*0.5, 0, 0)];

//mask path that contains circle with 20px on each edge
UIBezierPath *endPath = [UIBezierPath bezierPathWithOvalInRect:CGRectMake(edgeSize, edgeSize, viewSize-edgeSize*2.0, viewSize-edgeSize*2.0)];

//do outer join by applying we are cutting a hole in the original path
[startPath appendPath:basePath];
[endPath appendPath:basePath];

//now prepare our mask layer 
CAShapeLayer *maskLayer = [[CAShapeLayer alloc] init];
//set it size to whole view 
maskLayer.frame = realBounds;
maskLayer.path = endPath.CGPath; // set fully visible path 
maskLayer.fillRule = kCAFillRuleEvenOdd; // this is the key to enable diff cut in both paths shapes
maskLayer.fillColor = [UIColor blackColor].CGColor;

//apply mask to layer
self.layer.mask = maskLayer; 
</code></pre>

<p>And because it is also a regular <code>CALayer</code> you can apply animation to it.</p>

<pre><code class="objc">
CABasicAnimation *maskAnimation = [CABasicAnimation animationWithKeyPath:kADVMaskAnimationKey];

maskAnimation.fromValue = startPath.CGPath;
maskAnimation.toValue = endPath.CGPath;
maskAnimation.duration = animationDuration;

[maskLayer addAnimation:maskAnimation forKey:kADVMaskAnimationKey];
</code></pre>

<p>One more thing, remember that animation apply changes to layer only while animating! if you for example set your layer scale to 0.5 and animate from 0.5 to 1.0 it will go back to 0.5 after animation so your layer need to set proper final value before animating. It&rsquo;s super important so keep this in mind.</p>

<p><b>Custom CALayer</b></p>

<p>My Pie Chart animation inside drilled button is done using custom build <code>CALayer</code> i had to do it because regular <code>CALayer</code> do not allow to animate Bezier path simply, it will interpolate value instead of redrawing it properly. You can find some detailed info about how it is done on <a href="http://blog.pixelingene.com/2012/02/animating-pie-slices-using-a-custom-calayer/">Pixel-in-Gene Blog</a> but the basic idea is to subclass <code>CALayer</code> and overwrite <code>-(void)drawInContext:(CGContextRef)ctx</code> which is called every time property changes. Here is detailed custom pie chart implementation:</p>

<pre><code class="objc">- (void)drawInContext:(CGContextRef)ctx
{

    CGPoint center = CGPointMake(self.bounds.size.width*0.5, self.bounds.size.height*0.5);
    CGFloat radius = fminf(center.x, center.y);
    CGContextBeginPath(ctx);
    CGContextMoveToPoint(ctx, center.x, center.y);
    CGPoint p1 = CGPointMake(center.x + radius * cosf(self.startAngle), center.y + radius * sinf(self.startAngle));
    CGContextAddLineToPoint(ctx, p1.x, p1.y);

    BOOL clockwise = self.startAngle &gt; self.endAngle;

    CGContextAddArc(ctx, center.x, center.y, radius, self.startAngle, self.endAngle, clockwise);
    CGContextClosePath(ctx);

    CGContextSetFillColorWithColor(ctx, self.fillColor);
    CGContextSetStrokeColorWithColor(ctx, [UIColor clearColor].CGColor);
    CGContextSetLineWidth(ctx, 0);
    CGContextDrawPath(ctx, kCGPathFillStroke);
}
</code></pre>

<p>As you can see it&rsquo;s similar to regular <code>drawRect:</code> call but in <code>CALayer</code> we need to tell it actually when to redraw. We do this by overwriting <code>+ (BOOL)needsDisplayForKey:(NSString *)key;</code> method in our custom class. This will tell the layer to redraw every time our color or angle changes even when animated.</p>

<p>Another thing that is done here is automatic animation generation for every property we have. To do this simply overwrite <code>- (id<CAAction>)actionForKey:(NSString *)event</code> and compare the event key with our property by using <code>[event isEqualToString:NSStringFromSelector(@selector(propertyToCompare))]</code> and fire up animation if we detect property that we want to animate.</p>

<p>One more thing&hellip; see the <code>@dynamic</code> keyword on properties instead of automatically generated <code>@synthesize</code> we do this because CoreAnimation will create sub values and dynamically update original one while animating the property. Basically if you used CoreData you should be familiar with it, if not the simplest explanation to <code>@dynamic</code> is that by using it we tell compiler that our <b>setter</b> and <b>getter</b> will be generated outside the class and can be dynamically created instead of static compile time code generation in class.</p>

<p>And that&rsquo;s all for today, as always <a href="https://github.com/noxytrux/AdvancedCoreAnimations">source code</a> available on my github. Next time i will use this code to finish this app using <code>Core Audio</code> as another blog topic.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Camera and iPhone]]></title>
    <link href="http://noxytrux.github.io/blog/2014/12/20/camera-and-iphone/"/>
    <updated>2014-12-20T15:36:32+01:00</updated>
    <id>http://noxytrux.github.io/blog/2014/12/20/camera-and-iphone</id>
    <content type="html"><![CDATA[<p>It&rsquo;s been a while since i wrote last post but it is time to cover another topic. I start seeing more and more Instagram like apps and as i have to put some camera preview in one of the last apps i was developing. I come to idea that is it a good thing to talk about. Basically if you want to use camera in you app you are using <code>UIImagePickerController</code> and in most cases is good enought. But what if you really need some nice custom animations and most of all custom size?</p>

<!--more-->


<p>Yeah you can say that if it comes to this we can use <code>AVCaptureVideoPreviewLayer</code> and this will cover all edge cases with animation and additional decorations , size etc. Ok you are right in this case your code will looks like this and we are done.</p>

<pre><code class="objc">- (void)setupCamera
{
    //setup session
    self.session = [[AVCaptureSession alloc] init];
    self.session.sessionPreset = AVCaptureSessionPresetPhoto;
    self.videoDevice = [self frontCamera];
    self.videoInput = [AVCaptureDeviceInput deviceInputWithDevice:self.videoDevice error:nil];
    [self.session addInput:self.videoInput];

    self.captureVideoPreviewLayer = [[AVCaptureVideoPreviewLayer alloc] initWithSession:self.session];
    CALayer *viewLayer = self.cameraButton.layer;

    [viewLayer setMasksToBounds:YES];

    self.captureVideoPreviewLayer.videoGravity = AVLayerVideoGravityResizeAspectFill;
    [[self.captureVideoPreviewLayer connection] setVideoOrientation: AVCaptureVideoOrientationLandscapeRight];
    self.captureVideoPreviewLayer.frame = [viewLayer bounds];
    [viewLayer addSublayer:self.captureVideoPreviewLayer];

    // Start Running the Session
    [self.session startRunning];

    //fade in 
    CABasicAnimation *anim = [CABasicAnimation animationWithKeyPath:@"opacity"];
    anim.fromValue = @(0);
    anim.toValue = @(1);
    anim.duration = 0.5f;
    [self.captureVideoPreviewLayer addAnimation:anim forKey:nil];
}
</code></pre>

<p>But we don&rsquo;t want it to be easy right? well let&rsquo;s say we need some effect like in instagram to make it simple let&rsquo;s use saturation:</p>

<p><img class="center" src="/assets/images/CustomCamera.gif" width="400"></p>

<p>I&rsquo;m using here OpenGL <code>GLKitViewController</code> attached to my rootViewController with some custom shader.</p>

<p>Advantages:</p>

<p>-Fully customizable<br/>
-Whatever size i want<br/>
-Can apply CAAnimation, UIView aniamtion etc<br/>
-It&rsquo;s bloody fast</p>

<p>Of course i&rsquo;m not telling this is the best option possible i saw some benchmarks revealing that Accelerate framework from Apple can win i some cases. This is a good moment to talk about something i saw that many developers that are not involved in gamedev thought that learning OpenGL is wast of time, which is not true as knowing how GPU works and how to program it gives you almost endless possibilities and in many cases the best performance possible. Trust me i&rsquo;m an engineer :)</p>

<p>Ok enought talking show me some code: (I&rsquo;m assuming that you know the basics of OpenGL ES, if not there are some good <a href="http://www.raywenderlich.com/3664/opengl-tutorial-for-ios-opengl-es-2-0">tutorials</a> that explaing it)</p>

<p>Let&rsquo;s start with setting up our scene, we need main controller with some GLKitViewController attached to it by embed segue using container.</p>

<p><img class="center" src="/assets/images/scene.png" width="700"></p>

<p>Now we need to handle the segue in our main controller</p>

<pre><code class="objc">- (void)prepareForSegue:(UIStoryboardSegue *)segue sender:(id)sender
{
    if ([segue.identifier isEqualToString:MDPSegueIdentifiers.customCamera]) {

        MPDCustomCameraViewController *cameraController = segue.destinationViewController;

        self.cameraController = cameraController;

        [self decorateCameraController:cameraController];
        [self animateCameraController:cameraController];

        [cameraController startCameraCapture];
    }
}
</code></pre>

<p>Both function are not important right now. One of it simply animate the camera view and second one mask it to the shape of the circle. Now let&rsquo;s talk about our heart of app.</p>

<p>To render Camera output in OpenGL we need to capture it texture somehow. Gladly Apple AVCaptureSession allow us to bind camera output directly to textures using <a href="http://en.wikipedia.org/wiki/YCbCr">YCbCr</a> color space.</p>

<p>First we need to setup OpenGL view:</p>

<pre><code class="objc">- (void)viewDidLoad {

    [super viewDidLoad];

    self.context = [[EAGLContext alloc] initWithAPI:kEAGLRenderingAPIOpenGLES2];

    if (!self.context) {
        NSLog(@"Failed to create ES context");
    }

    //create Opengl view we are processing view update on demand so pause the view
    self.glView = (GLKView *)self.view;
    self.glView.context = self.context;
    self.glView.drawableDepthFormat = GLKViewDrawableDepthFormat24;
    self.preferredFramesPerSecond = 60;

    self.glView.delegate = self;

    self.view.backgroundColor = [UIColor whiteColor];

    self.retinaScaleFactor = [[UIScreen mainScreen] nativeScale];

    self.view.contentScaleFactor = self.retinaScaleFactor;

    _sessionPreset = AVCaptureSessionPreset640x480;

    [self setupGL];
    [self setupAVCapture];
}
</code></pre>

<p>Now lets setup AVSession to preapre data capture into 2 textures one containing chroma and second one luma as described in YCbCr spec.</p>

<pre><code class="objc"> CVReturn err = CVOpenGLESTextureCacheCreate(kCFAllocatorDefault, NULL, _context, NULL, &amp;_videoTextureCache);
    if (err)
    {
        NSLog(@"Error at CVOpenGLESTextureCacheCreate %d", err);
        return;
    }

    //-- Setup Capture Session.
    self.session = [[AVCaptureSession alloc] init];
    [self.session beginConfiguration];

    //-- Set preset session size.
    [self.session setSessionPreset:_sessionPreset];

    //-- Creata a video device and input from that Device.  Add the input to the capture session.
    NSArray *devices = [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo];

    AVCaptureDevice * videoDevice = nil;

    for (AVCaptureDevice *device in devices) {

        if ([device hasMediaType:AVMediaTypeVideo]) {

            if ([device position] == AVCaptureDevicePositionFront) {
                videoDevice = device;
                break;
            }
        }
    }

    if(videoDevice == nil)
        assert(0);

    //-- Add the device to the session.
    NSError *error;
    AVCaptureDeviceInput *input = [AVCaptureDeviceInput deviceInputWithDevice:videoDevice error:&amp;error];
    if(error)
        assert(0);

    [self.session addInput:input];

    //-- Create the output for the capture session.
    AVCaptureVideoDataOutput * dataOutput = [[AVCaptureVideoDataOutput alloc] init];
    [dataOutput setAlwaysDiscardsLateVideoFrames:YES]; // Probably want to set this to NO when recording

    //-- Set to YUV420.
    [dataOutput setVideoSettings:[NSDictionary dictionaryWithObject:[NSNumber numberWithInt:kCVPixelFormatType_420YpCbCr8BiPlanarFullRange]
                                                             forKey:(id)kCVPixelBufferPixelFormatTypeKey]]; // Necessary for manual preview

    // Set dispatch to be on the main thread so OpenGL can do things with the data
    [dataOutput setSampleBufferDelegate:self queue:dispatch_get_main_queue()];

    [self.session addOutput:dataOutput];
    [self.session commitConfiguration];
</code></pre>

<p>This code simply setup camera output to render in YCbCr mode which will return in delegate data for 2 textues. Also we setup device to front camera if you want to use Back camera modify this code:</p>

<pre><code class="objc">AVCaptureDevice * videoDevice = nil;

    for (AVCaptureDevice *device in devices) {

        if ([device hasMediaType:AVMediaTypeVideo]) {

            if ([device position] == AVCaptureDevicePositionFront) {
                videoDevice = device;
                break;
            }
        }
    }
</code></pre>

<p>And replace <code>AVCaptureDevicePositionFront</code> with <code>AVCaptureDevicePositionBack</code>.</p>

<p>Now most important function this one will capture our output and generate new textues for shader every frame.</p>

<pre><code class="objc">- (void)captureOutput:(AVCaptureOutput *)captureOutput
didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer
       fromConnection:(AVCaptureConnection *)connection
{
    CVReturn err;
    CVImageBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
    size_t width = CVPixelBufferGetWidth(pixelBuffer);
    size_t height = CVPixelBufferGetHeight(pixelBuffer);

    if (!_videoTextureCache)
    {
        NSLog(@"No video texture cache");
        return;
    }

    [self cleanUpTextures];

    // CVOpenGLESTextureCacheCreateTextureFromImage will create GLES texture
    // optimally from CVImageBufferRef.

    // Y-plane
    glActiveTexture(GL_TEXTURE0);
    err = CVOpenGLESTextureCacheCreateTextureFromImage(kCFAllocatorDefault,
                                                       _videoTextureCache,
                                                       pixelBuffer,
                                                       NULL,
                                                       GL_TEXTURE_2D,
                                                       GL_RED_EXT,
                                                       (int)width,
                                                       (int)height,
                                                       GL_RED_EXT,
                                                       GL_UNSIGNED_BYTE,
                                                       0,
                                                       &amp;_lumaTexture);
    if (err)
    {
        NSLog(@"Error at CVOpenGLESTextureCacheCreateTextureFromImage %d", err);
    }

    glBindTexture(CVOpenGLESTextureGetTarget(_lumaTexture), CVOpenGLESTextureGetName(_lumaTexture));
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);

    // UV-plane
    glActiveTexture(GL_TEXTURE1);
    err = CVOpenGLESTextureCacheCreateTextureFromImage(kCFAllocatorDefault,
                                                       _videoTextureCache,
                                                       pixelBuffer,
                                                       NULL,
                                                       GL_TEXTURE_2D,
                                                       GL_RG_EXT,
                                                       (int)(width * 0.5),
                                                       (int)(height * 0.5),
                                                       GL_RG_EXT,
                                                       GL_UNSIGNED_BYTE,
                                                       1,
                                                       &amp;_chromaTexture);
    if (err)
    {
        NSLog(@"Error at CVOpenGLESTextureCacheCreateTextureFromImage %d", err);
    }

    glBindTexture(CVOpenGLESTextureGetTarget(_chromaTexture), CVOpenGLESTextureGetName(_chromaTexture));
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);
}
</code></pre>

<p>Rendering is quiet simple:</p>

<pre><code class="objc">- (void)glkView:(GLKView *)view drawInRect:(CGRect)rect
{
    glViewport(0, 0,
               rect.size.width*self.retinaScaleFactor,
               rect.size.height*self.retinaScaleFactor);

    glClearColor(0.5f, 0.5f, 0.5f, 1.0f);
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);

    if(_lumaTexture != 0 &amp;&amp; _chromaTexture != 0){

        glUseProgram(_shaderProgram);

        glActiveTexture(GL_TEXTURE0);
        glBindTexture(CVOpenGLESTextureGetTarget(_lumaTexture), CVOpenGLESTextureGetName(_lumaTexture));
        glUniform1i(_tex1Uniform,0);
        glActiveTexture(GL_TEXTURE1);
        glBindTexture(CVOpenGLESTextureGetTarget(_chromaTexture), CVOpenGLESTextureGetName(_chromaTexture));
        glUniform1i(_tex2Uniform,1);
        glUniformMatrix4fv(_matrixUniform, 1 ,false ,GLKMatrix4MakeRotation(0, 0, 0, 1).m);
        glUniform1f(_saturationUniform, _saturation);
        glDisable(GL_DEPTH_TEST);

        glVertexAttribPointer(ATTRIB_VERTEX, 2, GL_FLOAT, 0, 0, squareVertices);
        glEnableVertexAttribArray(ATTRIB_VERTEX);
        glVertexAttribPointer(ATTRIB_COORDS, 2, GL_FLOAT, 0, 0, textureVertices);
        glEnableVertexAttribArray(ATTRIB_COORDS);

        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);
    }
}
</code></pre>

<p>Simply set the viewport to whole view size and fireup shader that combines both textures into final color.</p>

<pre><code class="glsl">    mediump vec3 yuv;
    mediump vec3 rgb;

    yuv.x  = texture2D(SamplerY, texCoordVarying).r;
    yuv.yz = texture2D(SamplerUV, texCoordVarying).rg - vec2(0.5, 0.5);

    rgb = mat3( 1.0, 1.0, 1.0,
                0.0, -.18732, 1.8556,
                1.57481, -.46813, 0.0) * yuv;
</code></pre>

<p>One more thing thanks to shaders and realtime computing i was eg able to do relatime saturation filter, filter itself is quiet simple. You just take a output color as vector3 and get as result dot product of the color and some precomuted constant value vector</p>

<pre><code class="glsl">mediump vec3 gray = vec3(dot(rgb, vec3(0.2126, 0.7152, 0.0722)));
mediump vec3 outColor = mix(rgb, gray, saturationFactor);    
</code></pre>

<p>See that we use uniform to mix between gray and output color.</p>

<p>And that&rsquo;s all i&#8217;v got for today as always <A HREF="https://github.com/noxytrux/CustomCameraiPhone">full source code</A> available on my github. As you can see now you can apply multiple effects on you camera simply by writing some additional shaders or extending current one, and all works in realtime.</p>

<p>P.S. Remember that you need to run this on physical device to use camera as it is not available on simulator.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS8 weirdness Part4: URLSessionTask]]></title>
    <link href="http://noxytrux.github.io/blog/2014/10/05/ios8-weirdness-part4-urlsessiontask/"/>
    <updated>2014-10-05T16:25:48+02:00</updated>
    <id>http://noxytrux.github.io/blog/2014/10/05/ios8-weirdness-part4-urlsessiontask</id>
    <content type="html"><![CDATA[<p>This post is one of those short ones. Again I&rsquo;m confused how switching from one iOS to another may transform your day to a horror. So what is this all about, if you are typical user of <code>NSURLConnection</code> you should acutally know that Apple introduces <code>NSURLSession</code> as a new way to handle networking. And if somethings goes wrong you need to handle it somehow&hellip;</p>

<!--more-->


<p>And this is where all things starting to be tricky; typically in iOS7 when you get an error from your request you handle it in some fail block passing <code>NSError</code> so my today talk is actually about <code>NSError</code>. Sometimes you need to handle paritcular error in way that is more readable for your user eg.: connecting to VPN, Timeouting etc. So let&rsquo;s say the error that is contained in <code>NSError</code> class need to be modified to something more readable eg by using CODE and DOMAIN to specify what we should actually show to the user. But in most of cases NSError contains quiet good description and it&rsquo;s even localized! But not in iOS8&hellip;</p>

<p>Some time ago i saw empty <code>UIAlertView</code> in some of the app i was working on. So i start digging what acutally happens, few minutes with debugger show me that <code>NSURLSessionTask</code> retuns <code>NSError</code> if something fails but it do not contains <code>NSLocalizedDescriptionKey</code> in userInfo the key is acutally missing! And using localizedDescription property also retuns some very unfriendly messages. So lets see what you will see in debugger:</p>

<p>iOS7.1:</p>

<pre><code class="c++">(lldb) po self.userInfo
{
    NSErrorFailingURLKey = "https://yourservername.com:8080/login/";
    NSErrorFailingURLStringKey = "https://yourservername.com:8080/login/";
    NSLocalizedDescription = "A server with the specified hostname could not be found.";
    NSUnderlyingError = "Error Domain=kCFErrorDomainCFNetwork Code=-1003 \"A server with the specified hostname could not be found.\" UserInfo=0x7ce56df0 {NSErrorFailingURLKey=https://yourservername.com:8080/login/, NSErrorFailingURLStringKey=https://yourservername.com:8080/login/, NSLocalizedDescription=A server with the specified hostname could not be found.}";
}
</code></pre>

<p>As you can see we can find <code>NSLocalizedDescription</code> ther. Now let&rsquo;s try to run same code on iOS8:</p>

<pre><code class="c++">(lldb) po self.userInfo
{
    NSErrorFailingURLKey = "https://yourservername.com:8080/login/";
    NSErrorFailingURLStringKey = "https://yourservername.com:8080/login/";
    NSUnderlyingError = "Error Domain=kCFErrorDomainCFNetwork Code=-1003 \"The operation couldn\U2019t be completed. (kCFErrorDomainCFNetwork error -1003.)\" UserInfo=0x7c31fad0 {_kCFStreamErrorDomainKey=12, _kCFStreamErrorCodeKey=8}";
    "_kCFStreamErrorCodeKey" = 8;
    "_kCFStreamErrorDomainKey" = 12;
}
</code></pre>

<p>As i said no <code>NSLocalizedDescription</code> at all, the worst thing is that no matter error your connection will generate, it will be always called the same <b>&ldquo;The operation couldn&rsquo;t be completed.&rdquo;</b> So if you have some production build and you try to get some info from your user, you are in serious trouble. The only way to fix this for now is to build some extension that will convert CODE and DOMAIN to apropriate error message.</p>

<pre><code class="objc">@import UIKit;

//some typical error codes
const NSInteger kServerNotFoundCode = -1003;
const NSInteger kVPNCode = -1005;
const NSInteger kTimeoutCode = -1001;

@implementation NSError (Utilities)

- (void)getDisplayTitle:(NSString * __autoreleasing *)title message:(NSString * __autoreleasing *)message

{

    *title = NSLocalizedString(@"error_label_title",nil);

        if ([self.domain isEqualToString:NSURLErrorDomain] == YES) {

            switch( self.code ) {

                case kVPNCode :
                    *message = NSLocalizedString(@"error_label_message_vpnnotenabled", nil);
                    break;

                case kServerNotFoundCode: 
                    *message = NSLocalizedString(@"error_label_message_servernotfound", nil);
                    break;

                case kTimeoutCode:
                    *message = NSLocalizedString(@"error_label_message_requesttimeout", nil);
                    break;

                case default:
                    *message = self.localizedDescription;
                    break;
            }

        }
        else {

           // unknow error or we don't know how to handle this display ugly info.
           *message = self.localizedDescription;
        }

}
</code></pre>

<p>And how to use it:</p>

<pre><code class="objc">
- (void)failWithError:(NSError *)error
{

     NSString *title, *message;

    [error getDisplayTitle:&amp;title 
                   message:&amp;message];

    UIAlertView *alert = [[UIAlertView alloc] initWithTitle:title
                                                    message:message
                                                   delegate:nil
                                          cancelButtonTitle:NSLocalizedString(@"error_label_ok", nil)
                                          otherButtonTitles:nil];
    [alert show];
}
</code></pre>

<p>Of course you may need to handle way more cases here so it is good to check all <a href="https://developer.apple.com/library/mac/documentation/Cocoa/Reference/Foundation/Miscellaneous/Foundation_Constants/Reference/reference.html#//apple_ref/doc/uid/TP40003793-CH3g-SW40">code list</a> in documentation. In this example im using localization to support multiple languages by myself if you do not need to do that just simply type there your description.</p>

<p>And that&#8217;a all for today, next time some swift cool features.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Godrays for mobile devices]]></title>
    <link href="http://noxytrux.github.io/blog/2014/10/04/godrays-for-mobile-devices/"/>
    <updated>2014-10-04T18:00:41+02:00</updated>
    <id>http://noxytrux.github.io/blog/2014/10/04/godrays-for-mobile-devices</id>
    <content type="html"><![CDATA[<p>This time I want to talk about one of the best postprocess effect I saw in games. It&rsquo;s called <code>Godrays</code> or <code>Lightshafts</code> depents on people I&rsquo;m calling it godrays. Basically it&rsquo;s one of the most challenging effects due to it&rsquo;s horsepower consumption. Today i will show you how to implement low cost Godrays for your mobile game (or PC).</p>

<!--more-->


<p>After correct implementation you should see something like that:</p>

<p><img class="center" src="/assets/images/godrays_image.png" width="800"></p>

<p>Awesome isn&rsquo;t it? Everything you see comes from my custom 3D engine that i build some time ago and now i did some finetunings to see how much i can achieve with current mobile phones generation. Demo runs at stable 30FPS having godrays, shadows, full physic etc. If there are any heavy calculations im using dynamic viewport scalling to drop down pixels and save some CPU and GPU calculations.</p>

<p>But enought talking time to show some code, ah before we start keep in mind this is OpenGL ES 2.0 implementation and you can do it even better and faster using ES 3.0 which contains MRT (multiple render target) to save some render passes.</p>

<p>First we need to implement our FBO to store dynamic textures:</p>

<pre><code class="cpp">
#import &lt;Foundation/Foundation.h&gt;
#import &lt;OpenGLES/EAGL.h&gt;
#import &lt;OpenGLES/ES2/gl.h&gt;
#import &lt;OpenGLES/ES2/glext.h&gt;

#define glBindVertexArray glBindVertexArrayOES
#define glGenVertexArrays glGenVertexArraysOES
#define glDeleteVertexArrays glDeleteVertexArraysOES

#ifndef FBO_H
#define FBO_H
enum MRT_TYPE { FBO_2D_COLOR, FBO_CUBE_COLOR, FBO_2D_DEPTH, FBO_2D_DEPTH2 };

typedef struct {
    MRT_TYPE    type; 
    int         format; 
    GLenum      m_eAttachment;
    GLenum      eTarget;
} MRTLayer;

class FrameBufferObject {
public:
    MRTLayer own;

    FrameBufferObject();
    ~FrameBufferObject() {Destroy();}

    void Add(MRTLayer Current);
    bool CreateNormal(MRT_TYPE type, int format, GLuint width, GLuint height);
    bool CreateDepth(GLuint width, GLuint height);
    bool Create(GLuint width, GLuint height);
    void Destroy();

    void Begin(GLuint nFace);
    void End(GLuint nFace);

    void Bind(GLint unit, GLint index);
    void Unbind(GLint unit);

    GLuint getTextureHandle(int what)   {return m_nTexId;}
    GLuint getWidth()   {return m_nWidth;}
    GLuint getHeight()  {return m_nHeight;}
    bool   isError() {return !m_bUseFBO;}


    bool CheckStatus();
    GLuint      m_nTexId;

private:
    bool        m_bUseFBO;
    bool        m_bUseDepthBuffer;

    GLuint      m_nWidth, m_nHeight;
    GLuint      m_nFrameBufferHandle;
    GLuint      m_nDepthBufferHandle;
    GLenum      m_eTextureType;
    GLuint      m_oldBuffer;

};
#endif
</code></pre>

<p>It is acutally a cpp code because i port it from my old 3D Engine i did when i was working on windows.</p>

<pre><code class="cpp">#include "FrameBufferObject.h"

FrameBufferObject::FrameBufferObject()
{
    m_nFrameBufferHandle=0;
    m_nDepthBufferHandle=0;
    m_nTexId = 0;
    m_nWidth = 0;
    m_nHeight = 0;
    m_bUseFBO = true;
    m_oldBuffer = 0;
}

void FrameBufferObject::Destroy()
{
    glDeleteTextures(1, &amp;m_nTexId);
    glDeleteFramebuffers(1, &amp;m_nFrameBufferHandle);
    if(m_bUseDepthBuffer)
        glDeleteRenderbuffers(1, &amp;m_nDepthBufferHandle);

    m_nFrameBufferHandle=0;
    m_nDepthBufferHandle=0;
    m_nTexId = 0;
    m_nWidth = 0;
    m_nHeight = 0;
    m_bUseFBO = true;
}

void FrameBufferObject::Add(MRTLayer Current)
{
    own = Current;
}

void FrameBufferObject::Begin(GLuint nFace) 
{
    assert(nFace&lt;6);
    glViewport(0, 0, m_nWidth, m_nHeight);


        glGetIntegerv(GL_FRAMEBUFFER_BINDING, (GLint *) &amp;m_oldBuffer);
        glBindFramebuffer(GL_FRAMEBUFFER, m_nFrameBufferHandle);

}

void FrameBufferObject::End(GLuint nFace) 
{
        glBindFramebuffer(GL_FRAMEBUFFER, m_oldBuffer);
}

void FrameBufferObject::Bind(GLint unit, GLint index) 
{
    glActiveTexture(GL_TEXTURE0 + unit);
    glEnable(m_eTextureType);
    glBindTexture(m_eTextureType, m_nTexId);
}

void FrameBufferObject::Unbind(GLint unit) 
{
    glActiveTexture(GL_TEXTURE0 + unit);
    glBindTexture( m_eTextureType, 0 );
    glDisable(m_eTextureType);
}

bool FrameBufferObject::Create(GLuint width, GLuint height){
    return false;
};

bool FrameBufferObject::CreateNormal(MRT_TYPE type, int format, GLuint width, GLuint height)
{
    own.type = type;
    own.format = format;
    Destroy();


    m_nWidth = width;
    m_nHeight = height;
    m_bUseFBO = true;
    m_bUseDepthBuffer = false;
    m_eTextureType = GL_TEXTURE_2D;

    //this is very important on mobile devices! you need to keep tracking
    //original FBO that iOS creates for you while rendering scene.
    glGetIntegerv(GL_FRAMEBUFFER_BINDING, (GLint *) &amp;m_oldBuffer);


    glGenRenderbuffers(1, &amp;m_nDepthBufferHandle);
    glBindRenderbuffer(GL_RENDERBUFFER, m_nDepthBufferHandle);
    glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT16, width, height);
    glBindRenderbuffer(GL_RENDERBUFFER, 0);

    glGenTextures(1, &amp;m_nTexId);
    glBindTexture(GL_TEXTURE_2D, m_nTexId);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);

    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);

    glTexParameteri(GL_TEXTURE_2D, GL_GENERATE_MIPMAP, GL_TRUE);
    glTexImage2D(GL_TEXTURE_2D, 0, format, width, height, 0,  GL_RGBA , GL_UNSIGNED_BYTE, 0);
    glBindTexture(GL_TEXTURE_2D, 0);

    glGenFramebuffers(1, &amp;m_nFrameBufferHandle);
    glBindFramebuffer(GL_FRAMEBUFFER, m_nFrameBufferHandle);
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, m_nTexId, 0);
    glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, m_nDepthBufferHandle);

    CheckStatus();

    glBindFramebuffer(GL_FRAMEBUFFER, m_oldBuffer);


    return true;
}

bool FrameBufferObject::CreateDepth(GLuint width, GLuint height)
{

    m_nWidth = width;
    m_nHeight = height;

    glGetIntegerv(GL_FRAMEBUFFER_BINDING, (GLint *) &amp;m_oldBuffer);

    glGenTextures(1, &amp;m_nTexId);
    glBindTexture(GL_TEXTURE_2D, m_nTexId);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);

    glTexImage2D(GL_TEXTURE_2D, 0, GL_DEPTH_STENCIL_OES, width, height, 
                 0,
                 GL_DEPTH_STENCIL_OES, 
                 GL_UNSIGNED_INT_24_8_OES, 
                 NULL);

    glBindTexture(GL_TEXTURE_2D, 0);

    glGenFramebuffers(1, &amp;m_nFrameBufferHandle);
    glBindFramebuffer(GL_FRAMEBUFFER, m_nFrameBufferHandle);
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_TEXTURE_2D, m_nTexId, 0);

    CheckStatus();

    glBindFramebuffer(GL_FRAMEBUFFER, m_oldBuffer);

    return true;
}

bool FrameBufferObject::CheckStatus()
{

    switch(glCheckFramebufferStatus(GL_FRAMEBUFFER)) {
        case GL_FRAMEBUFFER_COMPLETE:
            NSLog(@"GL_FRAMEBUFFER_COMPLETE_EXT ");
            return true;
            break;

        case GL_FRAMEBUFFER_INCOMPLETE_ATTACHMENT:
            NSLog(@"GL_FRAMEBUFFER_INCOMPLETE_ATTACHMENT_EXT ");

            break;

        case GL_FRAMEBUFFER_INCOMPLETE_MISSING_ATTACHMENT:
            NSLog(@"GL_FRAMEBUFFER_INCOMPLETE_MISSING_ATTACHMENT_EXT ");

            break;
#if TARGET_OS_IPHONE    
        case GL_FRAMEBUFFER_INCOMPLETE_DIMENSIONS:
            NSLog(@"GL_FRAMEBUFFER_INCOMPLETE_DIMENSIONS_EXT ");

            break;
#endif
        case GL_FRAMEBUFFER_UNSUPPORTED:
            NSLog(@"GL_FRAMEBUFFER_UNSUPPORTED_EXT ");

            break;

    }

    return false;
}
</code></pre>

<p>Now we need some code that will help us render, find and transform sun position to screen space.</p>

<pre><code class="cpp">
//this should be somehow dynamic but it's tutorial so I'm hardcoding this :)
float GODRAY_X = 568;
float GODRAY_Y = 320;
float RETINA_SCALE = 2.0f;
float shaftX;
float shaftY;

static const GLfloat squareVertices[] = {
    -GODRAY_X, -GODRAY_Y,
    GODRAY_X, -GODRAY_Y,
    -GODRAY_X,  GODRAY_Y,
    GODRAY_X,  GODRAY_Y,
};

static const GLfloat textureVertices[] = {
    0.0f, 0.0f,
    1.0f, 0.0f,
    0.0f, 1.0f,
    1.0f, 1.0f,
}; 
</code></pre>

<p>Now create our FBO that will keep depth and 2 another that will compose the final image by blurring downscaled image.</p>

<pre><code class="cpp">
FrameBufferObject *FBO;
FrameBufferObject *BFBO;
FrameBufferObject *BFBO2;

FBO = new FrameBufferObject;
FBO-&gt;CreateDepth(GODRAY_X, GODRAY_Y);

BFBO = new FrameBufferObject;
BFBO-&gt;CreateNormal(FBO_2D_COLOR, GL_RGBA, GODRAY_X, GODRAY_Y);

BFBO2 = new FrameBufferObject;
BFBO2-&gt;CreateNormal(FBO_2D_COLOR, GL_RGBA, GODRAY_X, GODRAY_Y);
</code></pre>

<p>Remember to dispose resources in dealloc!</p>

<pre><code class="objc">- (void)dealloc
{
    FBO-&gt;Destroy();
    delete FBO;

    BFBO-&gt;Destroy();
    delete BFBO; 

    BFBO2-&gt;Destroy();
    delete BFBO2; 
}
</code></pre>

<p>Now time to prepare data, render objects to FBO and compose it to final image</p>

<pre><code class="objc">
void getLightScreenCoor(xVec3 light, float &amp;uniformLightX, float &amp;uniformLightY)
{
    int viewport[4] = {0, 0, GODRAY_X, GODRAY_Y};
    GLKVector3 msun = GLKVector3Make(light.x, light.y, light.z);
    GLKVector3 win = GLKMathProject(msun, 
                                    ModelView, 
                                    Projection, 
                                    viewport);
    uniformLightX = win.x/GODRAY_X;
    uniformLightY = win.y/GODRAY_Y;
}

- (void)glkView:(GLKView *)view drawInRect:(CGRect)rect
{
    glClearColor(0.65f, 0.65f, 0.65f, 1.0f);

    FBO-&gt;Begin(0);
    {
        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);    

        //render your objects here and store their depth   

        ...      
    }
    FBO-&gt;End(0);   

    glViewport(0, 0, rect.size.width*RETINA_SCALE, rect.size.height*RETINA_SCALE);
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);

    //NOW RENDER YOUR DATA AGAIN TO OUTPUT FBO

    ...

    //PROPER LIGHTSHAFTS RENDERING

    //1st get sun direction vector, find it's position in 3D space and convert to 2D position
    xVec3 Sun = xVec3(-0.291719, -0.951882, -0.093922); 
    float sunsize = 1.23 * 9000 - 4000;
    Sun *= sunsize;

    //get 2D position
    getLightScreenCoor(Sun, shaftX, shaftY);

    Sun.normalize();
    float dotlight = eyeDirection.dot(Sun); //THIS IS USEFULL FOR CALCUALTING HOW MUCH GOD RAYS POSTPROCESS WE WANT TO APPLY

    Projection = GLKMatrix4MakeOrtho(-GODRAY_X, GODRAY_X, -GODRAY_Y, GODRAY_Y, 0.0, 1000.0);
    ModelView = GLKMatrix4Identity;

    glClear(GL_DEPTH_BUFFER_BIT);

    //NOW BUILD OUR SHAFTS TEXTURE

        //COMPUTE SHAFTS
        BFBO-&gt;Begin(0);
        {
            glUseProgram(ShaftShader-&gt;ShaderProgram);

            glActiveTexture(GL_TEXTURE0);
            glEnable(GL_TEXTURE_2D);
            glBindTexture(GL_TEXTURE_2D, FBO-&gt;m_nTexId);
            glUniformMatrix4fv(ShaftShader-&gt;uniforms[UNI_PROJECTION_MAT], 1 ,false , Projection.m);
            glUniformMatrix4fv(ShaftShader-&gt;uniforms[UNI_MODELVIEW_WORLD_MAT], 1 ,false , ModelView.m);
            glUniform1i(ShaftShader-&gt;uniforms[UNI_TEX0],0);
            glUniform2f(ShaftShader-&gt;uniforms[UNI_SCREEN_POS], shaftX, shaftY);
            glUniform1f(ShaftShader-&gt;uniforms[UNI_DOT_LIGHT], dotlight);
            glUniform3fv(ShaftShader-&gt;uniforms[UNI_LIGHT_COLOR], 1, m_SunColor.get());

            glVertexAttribPointer(ATTRIB_VERTEX, 2, GL_FLOAT, 0, 0, squareVertices);
            glEnableVertexAttribArray(ATTRIB_VERTEX);
            glVertexAttribPointer(ATTRIB_COORDS, 2, GL_FLOAT, 0, 0, textureVertices);
            glEnableVertexAttribArray(ATTRIB_COORDS);

            glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);

        }
        BFBO-&gt;End(0);

        //BLUR VERTICALLY
        BFBO2-&gt;Begin(0);
        {
            glUseProgram(blurShaderY-&gt;ShaderProgram);

            glActiveTexture(GL_TEXTURE0);
            glEnable(GL_TEXTURE_2D);
            glBindTexture(GL_TEXTURE_2D, BFBO-&gt;m_nTexId);
            glUniformMatrix4fv(blurShaderY-&gt;uniforms[UNI_PROJECTION_MAT], 1 ,false , Projection.m);
            glUniformMatrix4fv(blurShaderY-&gt;uniforms[UNI_MODELVIEW_WORLD_MAT], 1 ,false , ModelView.m);
            glUniform1i(blurShaderY-&gt;uniforms[UNI_TEX0],0);

            glVertexAttribPointer(ATTRIB_VERTEX, 2, GL_FLOAT, 0, 0, squareVertices);
            glEnableVertexAttribArray(ATTRIB_VERTEX);
            glVertexAttribPointer(ATTRIB_COORDS, 2, GL_FLOAT, 0, 0, textureVertices);
            glEnableVertexAttribArray(ATTRIB_COORDS);

            glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);
        }
        BFBO2-&gt;End(0);

        //BLUR HORIZONTALLY AND COMPOSE WITH CURRENT IMAGE

        glViewport(0, 0, rect.size.width*RETINA_SCALE, rect.size.height*RETINA_SCALE);

        glBlendFunc(GL_ONE, GL_ONE); //CHANGE TO ADDITIVE BLENDING

        glUseProgram(blurShaderX-&gt;ShaderProgram);

        glActiveTexture(GL_TEXTURE0);
        glEnable(GL_TEXTURE_2D);
        glBindTexture(GL_TEXTURE_2D, BFBO2-&gt;m_nTexId);
        glUniformMatrix4fv(blurShaderX-&gt;uniforms[UNI_PROJECTION_MAT], 1 ,false , Projection.m);
        glUniformMatrix4fv(blurShaderX-&gt;uniforms[UNI_MODELVIEW_WORLD_MAT], 1 ,false , ModelView.m);
        glUniform1i(blurShaderX-&gt;uniforms[UNI_TEX0],0);

        glVertexAttribPointer(ATTRIB_VERTEX, 2, GL_FLOAT, 0, 0, squareVertices);
        glEnableVertexAttribArray(ATTRIB_VERTEX);
        glVertexAttribPointer(ATTRIB_COORDS, 2, GL_FLOAT, 0, 0, textureVertices);
        glEnableVertexAttribArray(ATTRIB_COORDS);

        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);

        //CHANGE BACK TO REGULAR BLEND
        glBlendFunc(GL_ONE,GL_ONE_MINUS_SRC_ALPHA);

}
</code></pre>

<p>Ok, so that&rsquo;s basically all about the code, now we need to talk about shaders beacause they are the 80% of our success here.</p>

<p>As you can see, im rendering to &frac14; of screen to reduce rendering time and pixel shader cost, but it will not look good. So that&rsquo;s why im using blurred image it will hide any glitches that are created by our shafts shader and there is another reason for that. I&rsquo;m using only 15 samples per frame which is terrible low (comparing typically you use around 30-50samples) and makes the screen looks very sharpy and ugly so thats another thing we need to hide. And there is ofcourse another cool feature: we can get some simplified HDRR by doing this.</p>

<p>Blur shaders are simple, you can find this implementation in many places:</p>

<p>HORIZONTAL:</p>

<pre><code class="glsl">attribute vec4 position;
attribute vec2 inputTextureCoordinate;
uniform mat4 projection;
uniform mat4 modelViewWorld;

varying vec2 vTexCoord;

void main()
{
    gl_Position = projection * modelViewWorld * position;
    vTexCoord = inputTextureCoordinate.xy;
}
</code></pre>

<pre><code class="glsl">uniform sampler2D RTScene; 
varying lowp vec2 vTexCoord;

const lowp float blurSize = 1.0/160.0; 

void main(void)
{
    mediump vec4 sum = vec4(0.0);

    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y - 4.0*blurSize)) * 0.05;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y - 3.0*blurSize)) * 0.09;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y - 2.0*blurSize)) * 0.12;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y - blurSize)) * 0.15;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y)) * 0.16;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y + blurSize)) * 0.15;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y + 2.0*blurSize)) * 0.12;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y + 3.0*blurSize)) * 0.09;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y + 4.0*blurSize)) * 0.05;

    gl_FragColor = sum;
}
</code></pre>

<p>VERTICAL:</p>

<pre><code class="glsl">attribute vec4 position;
attribute vec2 inputTextureCoordinate;
uniform mat4 projection;
uniform mat4 modelViewWorld;

varying vec2 vTexCoord;

void main()
{
    gl_Position = projection * modelViewWorld * position;
    vTexCoord = inputTextureCoordinate.xy;
}
</code></pre>

<pre><code class="glsl">uniform sampler2D RTBlurH; 
varying lowp vec2 vTexCoord;

const lowp float blurSize = 1.0/240.0;

void main(void)
{
    mediump vec4 sum = vec4(0.0);

    sum += texture2D(RTBlurH, vec2(vTexCoord.x - 4.0*blurSize, vTexCoord.y)) * 0.05;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x - 3.0*blurSize, vTexCoord.y)) * 0.09;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x - 2.0*blurSize, vTexCoord.y)) * 0.12;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x - blurSize, vTexCoord.y)) * 0.15;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x, vTexCoord.y)) * 0.16;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x + blurSize, vTexCoord.y)) * 0.15;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x + 2.0*blurSize, vTexCoord.y)) * 0.12;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x + 3.0*blurSize, vTexCoord.y)) * 0.09;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x + 4.0*blurSize, vTexCoord.y)) * 0.05;

    gl_FragColor = sum;
}
</code></pre>

<p>Now it is time for our main shader, it uses some tricks that allow you to reduce calculation cost:</p>

<pre><code class="glsl">attribute vec4 position;
attribute vec2 inputTextureCoordinate;

uniform mat4 projection;
uniform mat4 modelViewWorld;
uniform vec2 lightSS;

varying vec2 textureCoordinate;
varying vec2 lightScreen;

void main()
{

    gl_Position = projection * modelViewWorld * position;

    lightScreen = lightSS;
    textureCoordinate = inputTextureCoordinate.xy;
}
</code></pre>

<pre><code class="glsl">precision mediump float;

uniform lowp sampler2D myTexture;
uniform lowp float dotlight;
uniform vec3 lightColor;

varying lowp vec2 textureCoordinate;
varying lowp vec2 lightScreen;

//here you can manipulate strenght, distance, and final result but current values should be good enought.
#define Density 0.25
#define Weight 0.3
#define Decay 0.99
#define Exposure 0.15

float illum = 0.0;
float illuminationDecay = 1.0;
vec2 deltaTexCoord = vec2( 0.0 );
vec2 texCoordp = vec2( 0.0 );

//this value should have exacly same number of Sample_It() calls.
#define NUM_SAMPLES 15.0
const float InvNumSamples = 1.0 / NUM_SAMPLES ;

void Sample_It(){
    texCoordp -= deltaTexCoord;
    //we need to offset step due to low precision on mobile normaly you should use 1.0
    float sampled = step( 0.99995 , texture2D(myTexture,texCoordp.st).r );
    illum += sampled * illuminationDecay * Weight * dotlight;
    illuminationDecay *= Decay;
}

void main(){
    texCoordp = textureCoordinate;
    deltaTexCoord = ( texCoordp - lightScreen ) * InvNumSamples * Density; 

    illum = 0.0;
    illuminationDecay = 1.0;

    Sample_It(); //1
    Sample_It(); //2
    Sample_It(); //3
    Sample_It(); //4
    Sample_It(); //5
    Sample_It(); //6
    Sample_It(); //7
    Sample_It(); //8
    Sample_It(); //9
    Sample_It(); //10
    Sample_It(); //11
    Sample_It(); //12
    Sample_It(); //13
    Sample_It(); //14
    Sample_It(); //15

    gl_FragColor = vec4( vec3( illum * Exposure ) * lightColor, 1.0 );

}
</code></pre>

<p>And voila! this will generate image with applied shafts to it. Ok so how it acutally works? <br/>
-You look at depth pixel by pixel  <br/>
-And check if the distance of it is bigger than specified offset (remember GPU saves depth as 0-1)   <br/>
-If there is any object on our way it will return 0 color so we have black pixel there     <br/>
-Otherwise white as we are poiting to infinity, multipled by specified color <br/>
-Repeat this N times (here I&rsquo;m using 15 samples but you may try to modify this eg on iPhone 4S i was using only 10samples ) <br/>
-Each time you repeat shift the result by specified direction vector (our sun position in screen space)</p>

<p>This image should explain this good enought:</p>

<p><img class="center" src="/assets/images/lightshafts-explain.jpg" width="800"></p>

<p>Here we can see final result:</p>

<p><div class="embed-video-container"><iframe src="http://www.youtube.com/embed/w6TJuDb0XDo" allowfullscreen></iframe></div></p>

<p>And that&rsquo;s quiet everything i have today, next time again iOS8 ;)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS UIButton backgroundColor you do it wrong...]]></title>
    <link href="http://noxytrux.github.io/blog/2014/09/25/ios-uibutton-backgroundcolor-you-do-it-wrong-dot-dot-dot/"/>
    <updated>2014-09-25T16:21:30+02:00</updated>
    <id>http://noxytrux.github.io/blog/2014/09/25/ios-uibutton-backgroundcolor-you-do-it-wrong-dot-dot-dot</id>
    <content type="html"><![CDATA[<p>This post is for those who are just starting they journey with iOS and Swift and want to learn some good practices on how to build this nice looking flat UI in modern iOS. Basically i want to talk about rectangle buttons that are filled with color. I saw some really bad implementations of <code>UIButton</code> and now, I want to present you a common mistake, i ofen see in many project even those mature ones.</p>

<!--more-->


<p>This image descibes our tutorial code, we have two button first one is done in bad habit and uses backgroundcolor and second one is done using background image property.</p>

<p><img class="center" src="/assets/images/look_buttons.png" width="300"></p>

<p>Basically backgroundColor property is good thing to use in Interface Builder to see how the button will looks like. But if you try to manage states you will end up with something like this:</p>

<pre><code class="Ruby">
@IBAction func buttonHightlight(sender: UIButton!) {

    sender.backgroundColor = UIColor(red: 255.0/255.0, green: 81.0/255.0, blue: 85.0/255.0, alpha: 0.5)    
}

@IBAction func buttonNormal(sender: UIButton!) {

    sender.backgroundColor = UIColor(red: 255.0/255.0, green: 81.0/255.0, blue: 85.0/255.0, alpha: 1.0)
}
</code></pre>

<p>It is event worst because your states in IB looks very similar to this:</p>

<p><img class="center" src="/assets/images/selector_ohno.png" width="300"></p>

<p>Madness and disaster, seriously that&rsquo;s not how you want to build your button, highlight state is build in and it&rsquo;s for free. And of course you only want to call function reponsible for pressing button no additional ones.</p>

<p>To do that we should use backgroundImage property of <code>UIButton</code>, but to not generate any additional assets let&rsquo;s build two extensions. First one will be reponsibe for generating <code>UIImage</code> from <code>UIColor</code> so we can set it directly from code.</p>

<pre><code class="Ruby">
import UIKit

extension UIImage {

    class func imageWithColor(color:UIColor?) -&gt; UIImage! {

        let rect = CGRectMake(0.0, 0.0, 1.0, 1.0);

        UIGraphicsBeginImageContextWithOptions(rect.size, false, 0)

        let context = UIGraphicsGetCurrentContext();

        if let color = color {

            color.setFill()
        }
        else {

            UIColor.whiteColor().setFill()
        }

        CGContextFillRect(context, rect);

        let image = UIGraphicsGetImageFromCurrentImageContext();
        UIGraphicsEndImageContext();

        return image;
    }

}
</code></pre>

<p>Second one is not required but is very usefull, and allow you to copy and paste color from eg gimp or photoshop. We are talking about UIColor extension that allow us to use hexadecimal strings as input value.</p>

<pre><code class="Ruby">
import UIKit

extension UIColor {

    class func colorWithHex(hexString: String?) -&gt; UIColor? {

        return colorWithHex(hexString, alpha: 1.0)
    }

    class func colorWithHex(hexString: String?, alpha: CGFloat) -&gt; UIColor? {

        if let hexString = hexString {

            var error : NSError? = nil

            let regexp = NSRegularExpression(pattern: "\\A#[0-9a-f]{6}\\z",
                options: .CaseInsensitive,
                error: &amp;error)

            let count = regexp.numberOfMatchesInString(hexString,
                options: .ReportProgress,
                range: NSMakeRange(0, countElements(hexString)))

            if count != 1 {

                return nil
            }

            var rgbValue : UInt32 = 0

            let scanner = NSScanner(string: hexString)

            scanner.scanLocation = 1
            scanner.scanHexInt(&amp;rgbValue)

            let red   = CGFloat( (rgbValue &amp; 0xFF0000) &gt;&gt; 16) / 255.0
            let green = CGFloat( (rgbValue &amp; 0xFF00) &gt;&gt; 8) / 255.0
            let blue  = CGFloat( (rgbValue &amp; 0xFF) ) / 255.0

            return UIColor(red: red, green: green, blue: blue, alpha: alpha)
        }

        return nil
    }

}
</code></pre>

<p>And that&rsquo;s it, so how do we set our good button ? like this:</p>

<pre><code class="Ruby">
//good button...
goodButton.backgroundColor = UIColor.clearColor() //reset IB Color
goodButton.setBackgroundImage(UIImage.imageWithColor(UIColor.colorWithHex("#66D269")), forState: .Normal)
goodButton.setBackgroundImage(UIImage.imageWithColor(UIColor.colorWithHex("#66D269", alpha: 0.5)), forState: .Highlighted)
</code></pre>

<p>So reassuming, do not use <code>IBAction</code> to modify button color, hightlight state etc. Use backgroundImage and if you don&rsquo;t want to use assets build them from code using extensions. As always <a href="https://github.com/noxytrux/ButtonsGoodPracticeTutorial">source code</a> on my github page.</p>
]]></content>
  </entry>
  
</feed>
