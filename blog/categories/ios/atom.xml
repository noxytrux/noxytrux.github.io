<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ios | Marcin Pędzimąż]]></title>
  <link href="http://noxytrux.github.io/blog/categories/ios/atom.xml" rel="self"/>
  <link href="http://noxytrux.github.io/"/>
  <updated>2014-12-20T22:49:17+01:00</updated>
  <id>http://noxytrux.github.io/</id>
  <author>
    <name><![CDATA[Marcin Pędzimąż]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Camera and iPhone]]></title>
    <link href="http://noxytrux.github.io/blog/2014/12/20/camera-and-iphone/"/>
    <updated>2014-12-20T15:36:32+01:00</updated>
    <id>http://noxytrux.github.io/blog/2014/12/20/camera-and-iphone</id>
    <content type="html"><![CDATA[<p>It&rsquo;s been a while since i wrote last post but it is time to cover another topic. I start seeing more and more Instagram like apps and as i have to put some camera preview in one of the last apps i was developing. I come to idea that is it a good thing to talk about. Basically if you want to use camera in you app you are using <code>UIImagePickerController</code> and in most cases is good enought. But what if you really need some nice custom animations and most of all custom size?</p>

<!--more-->


<p>Yeah you can say that if it comes to this we can use <code>AVCaptureVideoPreviewLayer</code> and this will cover all edge cases with animation and additional decorations , size etc. Ok you are right in this case your code will looks like this and we are done.</p>

<pre><code class="objc">- (void)setupCamera
{
    //setup session
    self.session = [[AVCaptureSession alloc] init];
    self.session.sessionPreset = AVCaptureSessionPresetPhoto;
    self.videoDevice = [self frontCamera];
    self.videoInput = [AVCaptureDeviceInput deviceInputWithDevice:self.videoDevice error:nil];
    [self.session addInput:self.videoInput];

    self.captureVideoPreviewLayer = [[AVCaptureVideoPreviewLayer alloc] initWithSession:self.session];
    CALayer *viewLayer = self.cameraButton.layer;

    [viewLayer setMasksToBounds:YES];

    self.captureVideoPreviewLayer.videoGravity = AVLayerVideoGravityResizeAspectFill;
    [[self.captureVideoPreviewLayer connection] setVideoOrientation: AVCaptureVideoOrientationLandscapeRight];
    self.captureVideoPreviewLayer.frame = [viewLayer bounds];
    [viewLayer addSublayer:self.captureVideoPreviewLayer];

    // Start Running the Session
    [self.session startRunning];

    //fade in 
    CABasicAnimation *anim = [CABasicAnimation animationWithKeyPath:@"opacity"];
    anim.fromValue = @(0);
    anim.toValue = @(1);
    anim.duration = 0.5f;
    [self.captureVideoPreviewLayer addAnimation:anim forKey:nil];
}
</code></pre>

<p>But we don&rsquo;t want it to be easy right? well let&rsquo;s say we need some effect like in instagram to make it simple let&rsquo;s use saturation:</p>

<p><img class="center" src="/assets/images/CustomCamera.gif" width="400"></p>

<p>I&rsquo;m using here OpenGL <code>GLKitViewController</code> attached to my rootViewController with some custom shader.</p>

<p>Advantages:</p>

<p>-Fully customizable<br/>
-Whatever size i want<br/>
-Can apply CAAnimation, UIView aniamtion etc<br/>
-It&rsquo;s bloody fast</p>

<p>Of course i&rsquo;m not telling this is the best option possible i saw some benchmarks revealing that Accelerate framework from Apple can win i some cases. This is a good moment to talk about something i saw that many developers that are not involved in gamedev thought that learning OpenGL is wast of time, which is not true as knowing how GPU works and how to program it gives you almost endless possibilities and in many cases the best performance possible. Trust me i&rsquo;m an engineer :)</p>

<p>Ok enought talking show me some code: (I&rsquo;m assuming that you know the basics of OpenGL ES, if not there are some good <a href="http://www.raywenderlich.com/3664/opengl-tutorial-for-ios-opengl-es-2-0">tutorials</a> that explaing it)</p>

<p>Let&rsquo;s start with setting up our scene, we need main controller with some GLKitViewController attached to it by embed segue using container.</p>

<p><img class="center" src="/assets/images/scene.png" width="700"></p>

<p>Now we need to handle the segue in our main controller</p>

<pre><code class="objc">- (void)prepareForSegue:(UIStoryboardSegue *)segue sender:(id)sender
{
    if ([segue.identifier isEqualToString:MDPSegueIdentifiers.customCamera]) {

        MPDCustomCameraViewController *cameraController = segue.destinationViewController;

        self.cameraController = cameraController;

        [self decorateCameraController:cameraController];
        [self animateCameraController:cameraController];

        [cameraController startCameraCapture];
    }
}
</code></pre>

<p>Both function are not important right now. One of it simply animate the camera view and second one mask it to the shape of the circle. Now let&rsquo;s talk about our heart of app.</p>

<p>To render Camera output in OpenGL we need to capture it texture somehow. Gladly Apple AVCaptureSession allow us to bind camera output directly to textures using <a href="http://en.wikipedia.org/wiki/YCbCr">YCbCr</a> color space.</p>

<p>First we need to setup OpenGL view:</p>

<pre><code class="objc">- (void)viewDidLoad {

    [super viewDidLoad];

    self.context = [[EAGLContext alloc] initWithAPI:kEAGLRenderingAPIOpenGLES2];

    if (!self.context) {
        NSLog(@"Failed to create ES context");
    }

    //create Opengl view we are processing view update on demand so pause the view
    self.glView = (GLKView *)self.view;
    self.glView.context = self.context;
    self.glView.drawableDepthFormat = GLKViewDrawableDepthFormat24;
    self.preferredFramesPerSecond = 60;

    self.glView.delegate = self;

    self.view.backgroundColor = [UIColor whiteColor];

    self.retinaScaleFactor = [[UIScreen mainScreen] nativeScale];

    self.view.contentScaleFactor = self.retinaScaleFactor;

    _sessionPreset = AVCaptureSessionPreset640x480;

    [self setupGL];
    [self setupAVCapture];
}
</code></pre>

<p>Now lets setup AVSession to preapre data capture into 2 textures one containing chroma and second one luma as described in YCbCr spec.</p>

<pre><code class="objc"> CVReturn err = CVOpenGLESTextureCacheCreate(kCFAllocatorDefault, NULL, _context, NULL, &amp;_videoTextureCache);
    if (err)
    {
        NSLog(@"Error at CVOpenGLESTextureCacheCreate %d", err);
        return;
    }

    //-- Setup Capture Session.
    self.session = [[AVCaptureSession alloc] init];
    [self.session beginConfiguration];

    //-- Set preset session size.
    [self.session setSessionPreset:_sessionPreset];

    //-- Creata a video device and input from that Device.  Add the input to the capture session.
    NSArray *devices = [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo];

    AVCaptureDevice * videoDevice = nil;

    for (AVCaptureDevice *device in devices) {

        if ([device hasMediaType:AVMediaTypeVideo]) {

            if ([device position] == AVCaptureDevicePositionFront) {
                videoDevice = device;
                break;
            }
        }
    }

    if(videoDevice == nil)
        assert(0);

    //-- Add the device to the session.
    NSError *error;
    AVCaptureDeviceInput *input = [AVCaptureDeviceInput deviceInputWithDevice:videoDevice error:&amp;error];
    if(error)
        assert(0);

    [self.session addInput:input];

    //-- Create the output for the capture session.
    AVCaptureVideoDataOutput * dataOutput = [[AVCaptureVideoDataOutput alloc] init];
    [dataOutput setAlwaysDiscardsLateVideoFrames:YES]; // Probably want to set this to NO when recording

    //-- Set to YUV420.
    [dataOutput setVideoSettings:[NSDictionary dictionaryWithObject:[NSNumber numberWithInt:kCVPixelFormatType_420YpCbCr8BiPlanarFullRange]
                                                             forKey:(id)kCVPixelBufferPixelFormatTypeKey]]; // Necessary for manual preview

    // Set dispatch to be on the main thread so OpenGL can do things with the data
    [dataOutput setSampleBufferDelegate:self queue:dispatch_get_main_queue()];

    [self.session addOutput:dataOutput];
    [self.session commitConfiguration];
</code></pre>

<p>This code simply setup camera output to render in YCbCr mode which will return in delegate data for 2 textues. Also we setup device to front camera if you want to use Back camera modify this code:</p>

<pre><code class="objc">AVCaptureDevice * videoDevice = nil;

    for (AVCaptureDevice *device in devices) {

        if ([device hasMediaType:AVMediaTypeVideo]) {

            if ([device position] == AVCaptureDevicePositionFront) {
                videoDevice = device;
                break;
            }
        }
    }
</code></pre>

<p>And replace <code>AVCaptureDevicePositionFront</code> with <code>AVCaptureDevicePositionBack</code>.</p>

<p>Now most important function this one will capture our output and generate new textues for shader every frame.</p>

<pre><code class="objc">- (void)captureOutput:(AVCaptureOutput *)captureOutput
didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer
       fromConnection:(AVCaptureConnection *)connection
{
    CVReturn err;
    CVImageBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
    size_t width = CVPixelBufferGetWidth(pixelBuffer);
    size_t height = CVPixelBufferGetHeight(pixelBuffer);

    if (!_videoTextureCache)
    {
        NSLog(@"No video texture cache");
        return;
    }

    [self cleanUpTextures];

    // CVOpenGLESTextureCacheCreateTextureFromImage will create GLES texture
    // optimally from CVImageBufferRef.

    // Y-plane
    glActiveTexture(GL_TEXTURE0);
    err = CVOpenGLESTextureCacheCreateTextureFromImage(kCFAllocatorDefault,
                                                       _videoTextureCache,
                                                       pixelBuffer,
                                                       NULL,
                                                       GL_TEXTURE_2D,
                                                       GL_RED_EXT,
                                                       (int)width,
                                                       (int)height,
                                                       GL_RED_EXT,
                                                       GL_UNSIGNED_BYTE,
                                                       0,
                                                       &amp;_lumaTexture);
    if (err)
    {
        NSLog(@"Error at CVOpenGLESTextureCacheCreateTextureFromImage %d", err);
    }

    glBindTexture(CVOpenGLESTextureGetTarget(_lumaTexture), CVOpenGLESTextureGetName(_lumaTexture));
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);

    // UV-plane
    glActiveTexture(GL_TEXTURE1);
    err = CVOpenGLESTextureCacheCreateTextureFromImage(kCFAllocatorDefault,
                                                       _videoTextureCache,
                                                       pixelBuffer,
                                                       NULL,
                                                       GL_TEXTURE_2D,
                                                       GL_RG_EXT,
                                                       (int)(width * 0.5),
                                                       (int)(height * 0.5),
                                                       GL_RG_EXT,
                                                       GL_UNSIGNED_BYTE,
                                                       1,
                                                       &amp;_chromaTexture);
    if (err)
    {
        NSLog(@"Error at CVOpenGLESTextureCacheCreateTextureFromImage %d", err);
    }

    glBindTexture(CVOpenGLESTextureGetTarget(_chromaTexture), CVOpenGLESTextureGetName(_chromaTexture));
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);
}
</code></pre>

<p>Rendering is quiet simple:</p>

<pre><code class="objc">- (void)glkView:(GLKView *)view drawInRect:(CGRect)rect
{
    glViewport(0, 0,
               rect.size.width*self.retinaScaleFactor,
               rect.size.height*self.retinaScaleFactor);

    glClearColor(0.5f, 0.5f, 0.5f, 1.0f);
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);

    if(_lumaTexture != 0 &amp;&amp; _chromaTexture != 0){

        glUseProgram(_shaderProgram);

        glActiveTexture(GL_TEXTURE0);
        glBindTexture(CVOpenGLESTextureGetTarget(_lumaTexture), CVOpenGLESTextureGetName(_lumaTexture));
        glUniform1i(_tex1Uniform,0);
        glActiveTexture(GL_TEXTURE1);
        glBindTexture(CVOpenGLESTextureGetTarget(_chromaTexture), CVOpenGLESTextureGetName(_chromaTexture));
        glUniform1i(_tex2Uniform,1);
        glUniformMatrix4fv(_matrixUniform, 1 ,false ,GLKMatrix4MakeRotation(0, 0, 0, 1).m);
        glUniform1f(_saturationUniform, _saturation);
        glDisable(GL_DEPTH_TEST);

        glVertexAttribPointer(ATTRIB_VERTEX, 2, GL_FLOAT, 0, 0, squareVertices);
        glEnableVertexAttribArray(ATTRIB_VERTEX);
        glVertexAttribPointer(ATTRIB_COORDS, 2, GL_FLOAT, 0, 0, textureVertices);
        glEnableVertexAttribArray(ATTRIB_COORDS);

        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);
    }
}
</code></pre>

<p>Simply set the viewport to whole view size and fireup shader that combines both textures into final color.</p>

<pre><code class="glsl">    mediump vec3 yuv;
    mediump vec3 rgb;

    yuv.x  = texture2D(SamplerY, texCoordVarying).r;
    yuv.yz = texture2D(SamplerUV, texCoordVarying).rg - vec2(0.5, 0.5);

    rgb = mat3( 1.0, 1.0, 1.0,
                0.0, -.18732, 1.8556,
                1.57481, -.46813, 0.0) * yuv;
</code></pre>

<p>One more thing thanks to shaders and realtime computing i was eg able to do relatime saturation filter, filter itself is quiet simple. You just take a output color as vector3 and get as result dot product of the color and some precomuted constant value vector</p>

<pre><code class="glsl">mediump vec3 gray = vec3(dot(rgb, vec3(0.2126, 0.7152, 0.0722)));
mediump vec3 outColor = mix(rgb, gray, saturationFactor);    
</code></pre>

<p>See that we use uniform to mix between gray and output color.</p>

<p>And that&rsquo;s all i&#8217;v got for today as always <A HREF="https://github.com/noxytrux/CustomCameraiPhone">full source code</A> available on my github. As you can see now you can apply multiple effects on you camera simply by writing some additional shaders or extending current one, and all works in realtime.</p>

<p>P.S. Remember that you need to run this on physical device to use camera as it is not available on simulator.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS8 weirdness Part4: URLSessionTask]]></title>
    <link href="http://noxytrux.github.io/blog/2014/10/05/ios8-weirdness-part4-urlsessiontask/"/>
    <updated>2014-10-05T16:25:48+02:00</updated>
    <id>http://noxytrux.github.io/blog/2014/10/05/ios8-weirdness-part4-urlsessiontask</id>
    <content type="html"><![CDATA[<p>This post is one of those short ones. Again I&rsquo;m confused how switching from one iOS to another may transform your day to a horror. So what is this all about, if you are typical user of <code>NSURLConnection</code> you should acutally know that Apple introduces <code>NSURLSession</code> as a new way to handle networking. And if somethings goes wrong you need to handle it somehow&hellip;</p>

<!--more-->


<p>And this is where all things starting to be tricky; typically in iOS7 when you get an error from your request you handle it in some fail block passing <code>NSError</code> so my today talk is actually about <code>NSError</code>. Sometimes you need to handle paritcular error in way that is more readable for your user eg.: connecting to VPN, Timeouting etc. So let&rsquo;s say the error that is contained in <code>NSError</code> class need to be modified to something more readable eg by using CODE and DOMAIN to specify what we should actually show to the user. But in most of cases NSError contains quiet good description and it&rsquo;s even localized! But not in iOS8&hellip;</p>

<p>Some time ago i saw empty <code>UIAlertView</code> in some of the app i was working on. So i start digging what acutally happens, few minutes with debugger show me that <code>NSURLSessionTask</code> retuns <code>NSError</code> if something fails but it do not contains <code>NSLocalizedDescriptionKey</code> in userInfo the key is acutally missing! And using localizedDescription property also retuns some very unfriendly messages. So lets see what you will see in debugger:</p>

<p>iOS7.1:</p>

<pre><code class="c++">(lldb) po self.userInfo
{
    NSErrorFailingURLKey = "https://yourservername.com:8080/login/";
    NSErrorFailingURLStringKey = "https://yourservername.com:8080/login/";
    NSLocalizedDescription = "A server with the specified hostname could not be found.";
    NSUnderlyingError = "Error Domain=kCFErrorDomainCFNetwork Code=-1003 \"A server with the specified hostname could not be found.\" UserInfo=0x7ce56df0 {NSErrorFailingURLKey=https://yourservername.com:8080/login/, NSErrorFailingURLStringKey=https://yourservername.com:8080/login/, NSLocalizedDescription=A server with the specified hostname could not be found.}";
}
</code></pre>

<p>As you can see we can find <code>NSLocalizedDescription</code> ther. Now let&rsquo;s try to run same code on iOS8:</p>

<pre><code class="c++">(lldb) po self.userInfo
{
    NSErrorFailingURLKey = "https://yourservername.com:8080/login/";
    NSErrorFailingURLStringKey = "https://yourservername.com:8080/login/";
    NSUnderlyingError = "Error Domain=kCFErrorDomainCFNetwork Code=-1003 \"The operation couldn\U2019t be completed. (kCFErrorDomainCFNetwork error -1003.)\" UserInfo=0x7c31fad0 {_kCFStreamErrorDomainKey=12, _kCFStreamErrorCodeKey=8}";
    "_kCFStreamErrorCodeKey" = 8;
    "_kCFStreamErrorDomainKey" = 12;
}
</code></pre>

<p>As i said no <code>NSLocalizedDescription</code> at all, the worst thing is that no matter error your connection will generate, it will be always called the same <b>&ldquo;The operation couldn&rsquo;t be completed.&rdquo;</b> So if you have some production build and you try to get some info from your user, you are in serious trouble. The only way to fix this for now is to build some extension that will convert CODE and DOMAIN to apropriate error message.</p>

<pre><code class="objc">@import UIKit;

//some typical error codes
const NSInteger kServerNotFoundCode = -1003;
const NSInteger kVPNCode = -1005;
const NSInteger kTimeoutCode = -1001;

@implementation NSError (Utilities)

- (void)getDisplayTitle:(NSString * __autoreleasing *)title message:(NSString * __autoreleasing *)message

{

    *title = NSLocalizedString(@"error_label_title",nil);

        if ([self.domain isEqualToString:NSURLErrorDomain] == YES) {

            switch( self.code ) {

                case kVPNCode :
                    *message = NSLocalizedString(@"error_label_message_vpnnotenabled", nil);
                    break;

                case kServerNotFoundCode: 
                    *message = NSLocalizedString(@"error_label_message_servernotfound", nil);
                    break;

                case kTimeoutCode:
                    *message = NSLocalizedString(@"error_label_message_requesttimeout", nil);
                    break;

                case default:
                    *message = self.localizedDescription;
                    break;
            }

        }
        else {

           // unknow error or we don't know how to handle this display ugly info.
           *message = self.localizedDescription;
        }

}
</code></pre>

<p>And how to use it:</p>

<pre><code class="objc">
- (void)failWithError:(NSError *)error
{

     NSString *title, *message;

    [error getDisplayTitle:&amp;title 
                   message:&amp;message];

    UIAlertView *alert = [[UIAlertView alloc] initWithTitle:title
                                                    message:message
                                                   delegate:nil
                                          cancelButtonTitle:NSLocalizedString(@"error_label_ok", nil)
                                          otherButtonTitles:nil];
    [alert show];
}
</code></pre>

<p>Of course you may need to handle way more cases here so it is good to check all <a href="https://developer.apple.com/library/mac/documentation/Cocoa/Reference/Foundation/Miscellaneous/Foundation_Constants/Reference/reference.html#//apple_ref/doc/uid/TP40003793-CH3g-SW40">code list</a> in documentation. In this example im using localization to support multiple languages by myself if you do not need to do that just simply type there your description.</p>

<p>And that&#8217;a all for today, next time some swift cool features.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Godrays for mobile devices]]></title>
    <link href="http://noxytrux.github.io/blog/2014/10/04/godrays-for-mobile-devices/"/>
    <updated>2014-10-04T18:00:41+02:00</updated>
    <id>http://noxytrux.github.io/blog/2014/10/04/godrays-for-mobile-devices</id>
    <content type="html"><![CDATA[<p>This time I want to talk about one of the best postprocess effect I saw in games. It&rsquo;s called <code>Godrays</code> or <code>Lightshafts</code> depents on people I&rsquo;m calling it godrays. Basically it&rsquo;s one of the most challenging effects due to it&rsquo;s horsepower consumption. Today i will show you how to implement low cost Godrays for your mobile game (or PC).</p>

<!--more-->


<p>After correct implementation you should see something like that:</p>

<p><img class="center" src="/assets/images/godrays_image.png" width="800"></p>

<p>Awesome isn&rsquo;t it? Everything you see comes from my custom 3D engine that i build some time ago and now i did some finetunings to see how much i can achieve with current mobile phones generation. Demo runs at stable 30FPS having godrays, shadows, full physic etc. If there are any heavy calculations im using dynamic viewport scalling to drop down pixels and save some CPU and GPU calculations.</p>

<p>But enought talking time to show some code, ah before we start keep in mind this is OpenGL ES 2.0 implementation and you can do it even better and faster using ES 3.0 which contains MRT (multiple render target) to save some render passes.</p>

<p>First we need to implement our FBO to store dynamic textures:</p>

<pre><code class="cpp">
#import &lt;Foundation/Foundation.h&gt;
#import &lt;OpenGLES/EAGL.h&gt;
#import &lt;OpenGLES/ES2/gl.h&gt;
#import &lt;OpenGLES/ES2/glext.h&gt;

#define glBindVertexArray glBindVertexArrayOES
#define glGenVertexArrays glGenVertexArraysOES
#define glDeleteVertexArrays glDeleteVertexArraysOES

#ifndef FBO_H
#define FBO_H
enum MRT_TYPE { FBO_2D_COLOR, FBO_CUBE_COLOR, FBO_2D_DEPTH, FBO_2D_DEPTH2 };

typedef struct {
    MRT_TYPE    type; 
    int         format; 
    GLenum      m_eAttachment;
    GLenum      eTarget;
} MRTLayer;

class FrameBufferObject {
public:
    MRTLayer own;

    FrameBufferObject();
    ~FrameBufferObject() {Destroy();}

    void Add(MRTLayer Current);
    bool CreateNormal(MRT_TYPE type, int format, GLuint width, GLuint height);
    bool CreateDepth(GLuint width, GLuint height);
    bool Create(GLuint width, GLuint height);
    void Destroy();

    void Begin(GLuint nFace);
    void End(GLuint nFace);

    void Bind(GLint unit, GLint index);
    void Unbind(GLint unit);

    GLuint getTextureHandle(int what)   {return m_nTexId;}
    GLuint getWidth()   {return m_nWidth;}
    GLuint getHeight()  {return m_nHeight;}
    bool   isError() {return !m_bUseFBO;}


    bool CheckStatus();
    GLuint      m_nTexId;

private:
    bool        m_bUseFBO;
    bool        m_bUseDepthBuffer;

    GLuint      m_nWidth, m_nHeight;
    GLuint      m_nFrameBufferHandle;
    GLuint      m_nDepthBufferHandle;
    GLenum      m_eTextureType;
    GLuint      m_oldBuffer;

};
#endif
</code></pre>

<p>It is acutally a cpp code because i port it from my old 3D Engine i did when i was working on windows.</p>

<pre><code class="cpp">#include "FrameBufferObject.h"

FrameBufferObject::FrameBufferObject()
{
    m_nFrameBufferHandle=0;
    m_nDepthBufferHandle=0;
    m_nTexId = 0;
    m_nWidth = 0;
    m_nHeight = 0;
    m_bUseFBO = true;
    m_oldBuffer = 0;
}

void FrameBufferObject::Destroy()
{
    glDeleteTextures(1, &amp;m_nTexId);
    glDeleteFramebuffers(1, &amp;m_nFrameBufferHandle);
    if(m_bUseDepthBuffer)
        glDeleteRenderbuffers(1, &amp;m_nDepthBufferHandle);

    m_nFrameBufferHandle=0;
    m_nDepthBufferHandle=0;
    m_nTexId = 0;
    m_nWidth = 0;
    m_nHeight = 0;
    m_bUseFBO = true;
}

void FrameBufferObject::Add(MRTLayer Current)
{
    own = Current;
}

void FrameBufferObject::Begin(GLuint nFace) 
{
    assert(nFace&lt;6);
    glViewport(0, 0, m_nWidth, m_nHeight);


        glGetIntegerv(GL_FRAMEBUFFER_BINDING, (GLint *) &amp;m_oldBuffer);
        glBindFramebuffer(GL_FRAMEBUFFER, m_nFrameBufferHandle);

}

void FrameBufferObject::End(GLuint nFace) 
{
        glBindFramebuffer(GL_FRAMEBUFFER, m_oldBuffer);
}

void FrameBufferObject::Bind(GLint unit, GLint index) 
{
    glActiveTexture(GL_TEXTURE0 + unit);
    glEnable(m_eTextureType);
    glBindTexture(m_eTextureType, m_nTexId);
}

void FrameBufferObject::Unbind(GLint unit) 
{
    glActiveTexture(GL_TEXTURE0 + unit);
    glBindTexture( m_eTextureType, 0 );
    glDisable(m_eTextureType);
}

bool FrameBufferObject::Create(GLuint width, GLuint height){
    return false;
};

bool FrameBufferObject::CreateNormal(MRT_TYPE type, int format, GLuint width, GLuint height)
{
    own.type = type;
    own.format = format;
    Destroy();


    m_nWidth = width;
    m_nHeight = height;
    m_bUseFBO = true;
    m_bUseDepthBuffer = false;
    m_eTextureType = GL_TEXTURE_2D;

    //this is very important on mobile devices! you need to keep tracking
    //original FBO that iOS creates for you while rendering scene.
    glGetIntegerv(GL_FRAMEBUFFER_BINDING, (GLint *) &amp;m_oldBuffer);


    glGenRenderbuffers(1, &amp;m_nDepthBufferHandle);
    glBindRenderbuffer(GL_RENDERBUFFER, m_nDepthBufferHandle);
    glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH_COMPONENT16, width, height);
    glBindRenderbuffer(GL_RENDERBUFFER, 0);

    glGenTextures(1, &amp;m_nTexId);
    glBindTexture(GL_TEXTURE_2D, m_nTexId);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);

    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);

    glTexParameteri(GL_TEXTURE_2D, GL_GENERATE_MIPMAP, GL_TRUE);
    glTexImage2D(GL_TEXTURE_2D, 0, format, width, height, 0,  GL_RGBA , GL_UNSIGNED_BYTE, 0);
    glBindTexture(GL_TEXTURE_2D, 0);

    glGenFramebuffers(1, &amp;m_nFrameBufferHandle);
    glBindFramebuffer(GL_FRAMEBUFFER, m_nFrameBufferHandle);
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, m_nTexId, 0);
    glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, m_nDepthBufferHandle);

    CheckStatus();

    glBindFramebuffer(GL_FRAMEBUFFER, m_oldBuffer);


    return true;
}

bool FrameBufferObject::CreateDepth(GLuint width, GLuint height)
{

    m_nWidth = width;
    m_nHeight = height;

    glGetIntegerv(GL_FRAMEBUFFER_BINDING, (GLint *) &amp;m_oldBuffer);

    glGenTextures(1, &amp;m_nTexId);
    glBindTexture(GL_TEXTURE_2D, m_nTexId);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);

    glTexImage2D(GL_TEXTURE_2D, 0, GL_DEPTH_STENCIL_OES, width, height, 
                 0,
                 GL_DEPTH_STENCIL_OES, 
                 GL_UNSIGNED_INT_24_8_OES, 
                 NULL);

    glBindTexture(GL_TEXTURE_2D, 0);

    glGenFramebuffers(1, &amp;m_nFrameBufferHandle);
    glBindFramebuffer(GL_FRAMEBUFFER, m_nFrameBufferHandle);
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_TEXTURE_2D, m_nTexId, 0);

    CheckStatus();

    glBindFramebuffer(GL_FRAMEBUFFER, m_oldBuffer);

    return true;
}

bool FrameBufferObject::CheckStatus()
{

    switch(glCheckFramebufferStatus(GL_FRAMEBUFFER)) {
        case GL_FRAMEBUFFER_COMPLETE:
            NSLog(@"GL_FRAMEBUFFER_COMPLETE_EXT ");
            return true;
            break;

        case GL_FRAMEBUFFER_INCOMPLETE_ATTACHMENT:
            NSLog(@"GL_FRAMEBUFFER_INCOMPLETE_ATTACHMENT_EXT ");

            break;

        case GL_FRAMEBUFFER_INCOMPLETE_MISSING_ATTACHMENT:
            NSLog(@"GL_FRAMEBUFFER_INCOMPLETE_MISSING_ATTACHMENT_EXT ");

            break;
#if TARGET_OS_IPHONE    
        case GL_FRAMEBUFFER_INCOMPLETE_DIMENSIONS:
            NSLog(@"GL_FRAMEBUFFER_INCOMPLETE_DIMENSIONS_EXT ");

            break;
#endif
        case GL_FRAMEBUFFER_UNSUPPORTED:
            NSLog(@"GL_FRAMEBUFFER_UNSUPPORTED_EXT ");

            break;

    }

    return false;
}
</code></pre>

<p>Now we need some code that will help us render, find and transform sun position to screen space.</p>

<pre><code class="cpp">
//this should be somehow dynamic but it's tutorial so I'm hardcoding this :)
float GODRAY_X = 568;
float GODRAY_Y = 320;
float RETINA_SCALE = 2.0f;
float shaftX;
float shaftY;

static const GLfloat squareVertices[] = {
    -GODRAY_X, -GODRAY_Y,
    GODRAY_X, -GODRAY_Y,
    -GODRAY_X,  GODRAY_Y,
    GODRAY_X,  GODRAY_Y,
};

static const GLfloat textureVertices[] = {
    0.0f, 0.0f,
    1.0f, 0.0f,
    0.0f, 1.0f,
    1.0f, 1.0f,
}; 
</code></pre>

<p>Now create our FBO that will keep depth and 2 another that will compose the final image by blurring downscaled image.</p>

<pre><code class="cpp">
FrameBufferObject *FBO;
FrameBufferObject *BFBO;
FrameBufferObject *BFBO2;

FBO = new FrameBufferObject;
FBO-&gt;CreateDepth(GODRAY_X, GODRAY_Y);

BFBO = new FrameBufferObject;
BFBO-&gt;CreateNormal(FBO_2D_COLOR, GL_RGBA, GODRAY_X, GODRAY_Y);

BFBO2 = new FrameBufferObject;
BFBO2-&gt;CreateNormal(FBO_2D_COLOR, GL_RGBA, GODRAY_X, GODRAY_Y);
</code></pre>

<p>Remember to dispose resources in dealloc!</p>

<pre><code class="objc">- (void)dealloc
{
    FBO-&gt;Destroy();
    delete FBO;

    BFBO-&gt;Destroy();
    delete BFBO; 

    BFBO2-&gt;Destroy();
    delete BFBO2; 
}
</code></pre>

<p>Now time to prepare data, render objects to FBO and compose it to final image</p>

<pre><code class="objc">
void getLightScreenCoor(xVec3 light, float &amp;uniformLightX, float &amp;uniformLightY)
{
    int viewport[4] = {0, 0, GODRAY_X, GODRAY_Y};
    GLKVector3 msun = GLKVector3Make(light.x, light.y, light.z);
    GLKVector3 win = GLKMathProject(msun, 
                                    ModelView, 
                                    Projection, 
                                    viewport);
    uniformLightX = win.x/GODRAY_X;
    uniformLightY = win.y/GODRAY_Y;
}

- (void)glkView:(GLKView *)view drawInRect:(CGRect)rect
{
    glClearColor(0.65f, 0.65f, 0.65f, 1.0f);

    FBO-&gt;Begin(0);
    {
        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);    

        //render your objects here and store their depth   

        ...      
    }
    FBO-&gt;End(0);   

    glViewport(0, 0, rect.size.width*RETINA_SCALE, rect.size.height*RETINA_SCALE);
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);

    //NOW RENDER YOUR DATA AGAIN TO OUTPUT FBO

    ...

    //PROPER LIGHTSHAFTS RENDERING

    //1st get sun direction vector, find it's position in 3D space and convert to 2D position
    xVec3 Sun = xVec3(-0.291719, -0.951882, -0.093922); 
    float sunsize = 1.23 * 9000 - 4000;
    Sun *= sunsize;

    //get 2D position
    getLightScreenCoor(Sun, shaftX, shaftY);

    Sun.normalize();
    float dotlight = eyeDirection.dot(Sun); //THIS IS USEFULL FOR CALCUALTING HOW MUCH GOD RAYS POSTPROCESS WE WANT TO APPLY

    Projection = GLKMatrix4MakeOrtho(-GODRAY_X, GODRAY_X, -GODRAY_Y, GODRAY_Y, 0.0, 1000.0);
    ModelView = GLKMatrix4Identity;

    glClear(GL_DEPTH_BUFFER_BIT);

    //NOW BUILD OUR SHAFTS TEXTURE

        //COMPUTE SHAFTS
        BFBO-&gt;Begin(0);
        {
            glUseProgram(ShaftShader-&gt;ShaderProgram);

            glActiveTexture(GL_TEXTURE0);
            glEnable(GL_TEXTURE_2D);
            glBindTexture(GL_TEXTURE_2D, FBO-&gt;m_nTexId);
            glUniformMatrix4fv(ShaftShader-&gt;uniforms[UNI_PROJECTION_MAT], 1 ,false , Projection.m);
            glUniformMatrix4fv(ShaftShader-&gt;uniforms[UNI_MODELVIEW_WORLD_MAT], 1 ,false , ModelView.m);
            glUniform1i(ShaftShader-&gt;uniforms[UNI_TEX0],0);
            glUniform2f(ShaftShader-&gt;uniforms[UNI_SCREEN_POS], shaftX, shaftY);
            glUniform1f(ShaftShader-&gt;uniforms[UNI_DOT_LIGHT], dotlight);
            glUniform3fv(ShaftShader-&gt;uniforms[UNI_LIGHT_COLOR], 1, m_SunColor.get());

            glVertexAttribPointer(ATTRIB_VERTEX, 2, GL_FLOAT, 0, 0, squareVertices);
            glEnableVertexAttribArray(ATTRIB_VERTEX);
            glVertexAttribPointer(ATTRIB_COORDS, 2, GL_FLOAT, 0, 0, textureVertices);
            glEnableVertexAttribArray(ATTRIB_COORDS);

            glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);

        }
        BFBO-&gt;End(0);

        //BLUR VERTICALLY
        BFBO2-&gt;Begin(0);
        {
            glUseProgram(blurShaderY-&gt;ShaderProgram);

            glActiveTexture(GL_TEXTURE0);
            glEnable(GL_TEXTURE_2D);
            glBindTexture(GL_TEXTURE_2D, BFBO-&gt;m_nTexId);
            glUniformMatrix4fv(blurShaderY-&gt;uniforms[UNI_PROJECTION_MAT], 1 ,false , Projection.m);
            glUniformMatrix4fv(blurShaderY-&gt;uniforms[UNI_MODELVIEW_WORLD_MAT], 1 ,false , ModelView.m);
            glUniform1i(blurShaderY-&gt;uniforms[UNI_TEX0],0);

            glVertexAttribPointer(ATTRIB_VERTEX, 2, GL_FLOAT, 0, 0, squareVertices);
            glEnableVertexAttribArray(ATTRIB_VERTEX);
            glVertexAttribPointer(ATTRIB_COORDS, 2, GL_FLOAT, 0, 0, textureVertices);
            glEnableVertexAttribArray(ATTRIB_COORDS);

            glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);
        }
        BFBO2-&gt;End(0);

        //BLUR HORIZONTALLY AND COMPOSE WITH CURRENT IMAGE

        glViewport(0, 0, rect.size.width*RETINA_SCALE, rect.size.height*RETINA_SCALE);

        glBlendFunc(GL_ONE, GL_ONE); //CHANGE TO ADDITIVE BLENDING

        glUseProgram(blurShaderX-&gt;ShaderProgram);

        glActiveTexture(GL_TEXTURE0);
        glEnable(GL_TEXTURE_2D);
        glBindTexture(GL_TEXTURE_2D, BFBO2-&gt;m_nTexId);
        glUniformMatrix4fv(blurShaderX-&gt;uniforms[UNI_PROJECTION_MAT], 1 ,false , Projection.m);
        glUniformMatrix4fv(blurShaderX-&gt;uniforms[UNI_MODELVIEW_WORLD_MAT], 1 ,false , ModelView.m);
        glUniform1i(blurShaderX-&gt;uniforms[UNI_TEX0],0);

        glVertexAttribPointer(ATTRIB_VERTEX, 2, GL_FLOAT, 0, 0, squareVertices);
        glEnableVertexAttribArray(ATTRIB_VERTEX);
        glVertexAttribPointer(ATTRIB_COORDS, 2, GL_FLOAT, 0, 0, textureVertices);
        glEnableVertexAttribArray(ATTRIB_COORDS);

        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);

        //CHANGE BACK TO REGULAR BLEND
        glBlendFunc(GL_ONE,GL_ONE_MINUS_SRC_ALPHA);

}
</code></pre>

<p>Ok, so that&rsquo;s basically all about the code, now we need to talk about shaders beacause they are the 80% of our success here.</p>

<p>As you can see, im rendering to &frac14; of screen to reduce rendering time and pixel shader cost, but it will not look good. So that&rsquo;s why im using blurred image it will hide any glitches that are created by our shafts shader and there is another reason for that. I&rsquo;m using only 15 samples per frame which is terrible low (comparing typically you use around 30-50samples) and makes the screen looks very sharpy and ugly so thats another thing we need to hide. And there is ofcourse another cool feature: we can get some simplified HDRR by doing this.</p>

<p>Blur shaders are simple, you can find this implementation in many places:</p>

<p>HORIZONTAL:</p>

<pre><code class="glsl">attribute vec4 position;
attribute vec2 inputTextureCoordinate;
uniform mat4 projection;
uniform mat4 modelViewWorld;

varying vec2 vTexCoord;

void main()
{
    gl_Position = projection * modelViewWorld * position;
    vTexCoord = inputTextureCoordinate.xy;
}
</code></pre>

<pre><code class="glsl">uniform sampler2D RTScene; 
varying lowp vec2 vTexCoord;

const lowp float blurSize = 1.0/160.0; 

void main(void)
{
    mediump vec4 sum = vec4(0.0);

    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y - 4.0*blurSize)) * 0.05;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y - 3.0*blurSize)) * 0.09;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y - 2.0*blurSize)) * 0.12;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y - blurSize)) * 0.15;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y)) * 0.16;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y + blurSize)) * 0.15;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y + 2.0*blurSize)) * 0.12;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y + 3.0*blurSize)) * 0.09;
    sum += texture2D(RTScene, vec2(vTexCoord.x, vTexCoord.y + 4.0*blurSize)) * 0.05;

    gl_FragColor = sum;
}
</code></pre>

<p>VERTICAL:</p>

<pre><code class="glsl">attribute vec4 position;
attribute vec2 inputTextureCoordinate;
uniform mat4 projection;
uniform mat4 modelViewWorld;

varying vec2 vTexCoord;

void main()
{
    gl_Position = projection * modelViewWorld * position;
    vTexCoord = inputTextureCoordinate.xy;
}
</code></pre>

<pre><code class="glsl">uniform sampler2D RTBlurH; 
varying lowp vec2 vTexCoord;

const lowp float blurSize = 1.0/240.0;

void main(void)
{
    mediump vec4 sum = vec4(0.0);

    sum += texture2D(RTBlurH, vec2(vTexCoord.x - 4.0*blurSize, vTexCoord.y)) * 0.05;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x - 3.0*blurSize, vTexCoord.y)) * 0.09;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x - 2.0*blurSize, vTexCoord.y)) * 0.12;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x - blurSize, vTexCoord.y)) * 0.15;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x, vTexCoord.y)) * 0.16;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x + blurSize, vTexCoord.y)) * 0.15;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x + 2.0*blurSize, vTexCoord.y)) * 0.12;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x + 3.0*blurSize, vTexCoord.y)) * 0.09;
    sum += texture2D(RTBlurH, vec2(vTexCoord.x + 4.0*blurSize, vTexCoord.y)) * 0.05;

    gl_FragColor = sum;
}
</code></pre>

<p>Now it is time for our main shader, it uses some tricks that allow you to reduce calculation cost:</p>

<pre><code class="glsl">attribute vec4 position;
attribute vec2 inputTextureCoordinate;

uniform mat4 projection;
uniform mat4 modelViewWorld;
uniform vec2 lightSS;

varying vec2 textureCoordinate;
varying vec2 lightScreen;

void main()
{

    gl_Position = projection * modelViewWorld * position;

    lightScreen = lightSS;
    textureCoordinate = inputTextureCoordinate.xy;
}
</code></pre>

<pre><code class="glsl">precision mediump float;

uniform lowp sampler2D myTexture;
uniform lowp float dotlight;
uniform vec3 lightColor;

varying lowp vec2 textureCoordinate;
varying lowp vec2 lightScreen;

//here you can manipulate strenght, distance, and final result but current values should be good enought.
#define Density 0.25
#define Weight 0.3
#define Decay 0.99
#define Exposure 0.15

float illum = 0.0;
float illuminationDecay = 1.0;
vec2 deltaTexCoord = vec2( 0.0 );
vec2 texCoordp = vec2( 0.0 );

//this value should have exacly same number of Sample_It() calls.
#define NUM_SAMPLES 15.0
const float InvNumSamples = 1.0 / NUM_SAMPLES ;

void Sample_It(){
    texCoordp -= deltaTexCoord;
    //we need to offset step due to low precision on mobile normaly you should use 1.0
    float sampled = step( 0.99995 , texture2D(myTexture,texCoordp.st).r );
    illum += sampled * illuminationDecay * Weight * dotlight;
    illuminationDecay *= Decay;
}

void main(){
    texCoordp = textureCoordinate;
    deltaTexCoord = ( texCoordp - lightScreen ) * InvNumSamples * Density; 

    illum = 0.0;
    illuminationDecay = 1.0;

    Sample_It(); //1
    Sample_It(); //2
    Sample_It(); //3
    Sample_It(); //4
    Sample_It(); //5
    Sample_It(); //6
    Sample_It(); //7
    Sample_It(); //8
    Sample_It(); //9
    Sample_It(); //10
    Sample_It(); //11
    Sample_It(); //12
    Sample_It(); //13
    Sample_It(); //14
    Sample_It(); //15

    gl_FragColor = vec4( vec3( illum * Exposure ) * lightColor, 1.0 );

}
</code></pre>

<p>And voila! this will generate image with applied shafts to it. Ok so how it acutally works? <br/>
-You look at depth pixel by pixel  <br/>
-And check if the distance of it is bigger than specified offset (remember GPU saves depth as 0-1)   <br/>
-If there is any object on our way it will return 0 color so we have black pixel there     <br/>
-Otherwise white as we are poiting to infinity, multipled by specified color <br/>
-Repeat this N times (here I&rsquo;m using 15 samples but you may try to modify this eg on iPhone 4S i was using only 10samples ) <br/>
-Each time you repeat shift the result by specified direction vector (our sun position in screen space)</p>

<p>This image should explain this good enought:</p>

<p><img class="center" src="/assets/images/lightshafts-explain.jpg" width="800"></p>

<p>Here we can see final result:</p>

<p><div class="embed-video-container"><iframe src="http://www.youtube.com/embed/w6TJuDb0XDo" allowfullscreen></iframe></div></p>

<p>And that&rsquo;s quiet everything i have today, next time again iOS8 ;)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS UIButton backgroundColor you do it wrong...]]></title>
    <link href="http://noxytrux.github.io/blog/2014/09/25/ios-uibutton-backgroundcolor-you-do-it-wrong-dot-dot-dot/"/>
    <updated>2014-09-25T16:21:30+02:00</updated>
    <id>http://noxytrux.github.io/blog/2014/09/25/ios-uibutton-backgroundcolor-you-do-it-wrong-dot-dot-dot</id>
    <content type="html"><![CDATA[<p>This post is for those who are just starting they journey with iOS and Swift and want to learn some good practices on how to build this nice looking flat UI in modern iOS. Basically i want to talk about rectangle buttons that are filled with color. I saw some really bad implementations of <code>UIButton</code> and now, I want to present you a common mistake, i ofen see in many project even those mature ones.</p>

<!--more-->


<p>This image descibes our tutorial code, we have two button first one is done in bad habit and uses backgroundcolor and second one is done using background image property.</p>

<p><img class="center" src="/assets/images/look_buttons.png" width="300"></p>

<p>Basically backgroundColor property is good thing to use in Interface Builder to see how the button will looks like. But if you try to manage states you will end up with something like this:</p>

<pre><code class="Ruby">
@IBAction func buttonHightlight(sender: UIButton!) {

    sender.backgroundColor = UIColor(red: 255.0/255.0, green: 81.0/255.0, blue: 85.0/255.0, alpha: 0.5)    
}

@IBAction func buttonNormal(sender: UIButton!) {

    sender.backgroundColor = UIColor(red: 255.0/255.0, green: 81.0/255.0, blue: 85.0/255.0, alpha: 1.0)
}
</code></pre>

<p>It is event worst because your states in IB looks very similar to this:</p>

<p><img class="center" src="/assets/images/selector_ohno.png" width="300"></p>

<p>Madness and disaster, seriously that&rsquo;s not how you want to build your button, highlight state is build in and it&rsquo;s for free. And of course you only want to call function reponsible for pressing button no additional ones.</p>

<p>To do that we should use backgroundImage property of <code>UIButton</code>, but to not generate any additional assets let&rsquo;s build two extensions. First one will be reponsibe for generating <code>UIImage</code> from <code>UIColor</code> so we can set it directly from code.</p>

<pre><code class="Ruby">
import UIKit

extension UIImage {

    class func imageWithColor(color:UIColor?) -&gt; UIImage! {

        let rect = CGRectMake(0.0, 0.0, 1.0, 1.0);

        UIGraphicsBeginImageContextWithOptions(rect.size, false, 0)

        let context = UIGraphicsGetCurrentContext();

        if let color = color {

            color.setFill()
        }
        else {

            UIColor.whiteColor().setFill()
        }

        CGContextFillRect(context, rect);

        let image = UIGraphicsGetImageFromCurrentImageContext();
        UIGraphicsEndImageContext();

        return image;
    }

}
</code></pre>

<p>Second one is not required but is very usefull, and allow you to copy and paste color from eg gimp or photoshop. We are talking about UIColor extension that allow us to use hexadecimal strings as input value.</p>

<pre><code class="Ruby">
import UIKit

extension UIColor {

    class func colorWithHex(hexString: String?) -&gt; UIColor? {

        return colorWithHex(hexString, alpha: 1.0)
    }

    class func colorWithHex(hexString: String?, alpha: CGFloat) -&gt; UIColor? {

        if let hexString = hexString {

            var error : NSError? = nil

            let regexp = NSRegularExpression(pattern: "\\A#[0-9a-f]{6}\\z",
                options: .CaseInsensitive,
                error: &amp;error)

            let count = regexp.numberOfMatchesInString(hexString,
                options: .ReportProgress,
                range: NSMakeRange(0, countElements(hexString)))

            if count != 1 {

                return nil
            }

            var rgbValue : UInt32 = 0

            let scanner = NSScanner(string: hexString)

            scanner.scanLocation = 1
            scanner.scanHexInt(&amp;rgbValue)

            let red   = CGFloat( (rgbValue &amp; 0xFF0000) &gt;&gt; 16) / 255.0
            let green = CGFloat( (rgbValue &amp; 0xFF00) &gt;&gt; 8) / 255.0
            let blue  = CGFloat( (rgbValue &amp; 0xFF) ) / 255.0

            return UIColor(red: red, green: green, blue: blue, alpha: alpha)
        }

        return nil
    }

}
</code></pre>

<p>And that&rsquo;s it, so how do we set our good button ? like this:</p>

<pre><code class="Ruby">
//good button...
goodButton.backgroundColor = UIColor.clearColor() //reset IB Color
goodButton.setBackgroundImage(UIImage.imageWithColor(UIColor.colorWithHex("#66D269")), forState: .Normal)
goodButton.setBackgroundImage(UIImage.imageWithColor(UIColor.colorWithHex("#66D269", alpha: 0.5)), forState: .Highlighted)
</code></pre>

<p>So reassuming, do not use <code>IBAction</code> to modify button color, hightlight state etc. Use backgroundImage and if you don&rsquo;t want to use assets build them from code using extensions. As always <a href="https://github.com/noxytrux/ButtonsGoodPracticeTutorial">source code</a> on my github page.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS8 Weirdness Part3: Laggy UICollectionView]]></title>
    <link href="http://noxytrux.github.io/blog/2014/09/25/ios8-weirdness-part3-laggy-uicollectionview/"/>
    <updated>2014-09-25T10:03:36+02:00</updated>
    <id>http://noxytrux.github.io/blog/2014/09/25/ios8-weirdness-part3-laggy-uicollectionview</id>
    <content type="html"><![CDATA[<p>Autolayouts, something that people love and hate. When Apple introduced them i had mixed felling about it, from one hand it reduces tons of code, also do a lot of calculation for you and of course it magically works! In other hand&hellip; it is terribly slow i still see my manual <code>setFrame:</code> at least 10 times faster than calculation something in AL but nothing for free.</p>

<!--more-->


<p></p>

<p>This post will not be long, the main problem is that as most of you may know using autolayout in scrollview is pain in ass terrible slow due to continous autolayout recalculation mostly if you use xib loaded view with AL attached to them. So why not to use UICollectionView better, faster, bigger, stronger. I have heavy collection view with very complicated cells (in terms of autolayout) but they are done pretty smart and work really fast and smooth, of course only in iOS7</p>

<p>After updating my ipad to iOS8 i encounter terrible lags while scrolling <code>UICollectionView</code> in my app. Again instruments bug haunting with tons of coffee shows me that my amazing <code>UICollectionView</code> starts acting exacly like ugly <code>UIScrollView</code> solution and recalculates cells autolayout over and over again!</p>

<p><img class="center" src="/assets/images/ohcomone.gif"></p>

<p>Gladly solution to that is pretty simple. If you watched <a href="https://developer.apple.com/videos/wwdc/2014/">WWDC 2014</a> about changes in collection view you propably noticed that Apple introduces some new Invaidation Context etc. But we do not need to implement all of that if you want to get your smooth and nice scrolling the only thing you need is to implement this method in your custom <code>UICollectionViewCell</code></p>

<pre><code class="objc">
- (UICollectionViewLayoutAttributes *)preferredLayoutAttributesFittingAttributes:(UICollectionViewLayoutAttributes *)layoutAttributes
{
    return layoutAttributes;
}
</code></pre>

<p>And you are done! Collection view again works as expected, it happens because this is called before autolayout calcualtion and it returns proper cell size so there is no need to trigger recalcualtion again. You should see some visual changes eg collection view cell will no more weirdly rescale from 1x1 to it desired scale.</p>

<p>That&rsquo;s all for now, next time some UIButton good practices for those who just start coding in iOS and want their flat buttons looks and works good.</p>
]]></content>
  </entry>
  
</feed>
